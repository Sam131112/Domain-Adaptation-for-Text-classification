{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de661b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from collections import Counter\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 16\n",
    "from transformers import AutoConfig,AutoModel\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer,DataCollatorWithPadding,\\\n",
    "                                        TrainingArguments, Trainer,default_data_collator,AdamW\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)\n",
    "from torch.utils.data import DataLoader,RandomSampler\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "from datasets import load_dataset,load_metric\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "f1_metric = load_metric(\"f1\")\n",
    "pr_metric = load_metric('precision')\n",
    "re_metric = load_metric('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc265be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.utils.data.dataset import ConcatDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af43607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,comb_data):\n",
    "        self.data = comb_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_data,mlm_data = self.data[idx][0],self.data[idx][1]\n",
    "        return token_data, mlm_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5024356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1569e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_slice(i):\n",
    "                    inputs = {\n",
    "                   'input_ids':i['input_ids'].squeeze(1).cuda(),\\\n",
    "                   'attention_mask':i['attention_mask'].squeeze(1).cuda(),\\\n",
    "                   'labels':i['labels'].squeeze(1).cuda(),\n",
    "                       }\n",
    "                    return inputs \n",
    "\n",
    "def preprocess_function(examples):\n",
    "        result = tokenizer(examples['text'], padding=\"max_length\", max_length=512, truncation=True)\n",
    "        return result\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def create_data_list(data1,data2):\n",
    "    data1 = [batch for step,batch in enumerate(data1)]\n",
    "    data2 = [batch for step,batch in enumerate(data2)]\n",
    "    store = []\n",
    "    if min(len(data1),len(data2))==len(data1):\n",
    "        small = data1\n",
    "        big = data2\n",
    "    else:\n",
    "        small = data2\n",
    "        big = data1\n",
    "    for i in range(len(small)):\n",
    "        store.append((big[i],small[i]))\n",
    "    for j in range(i,len(big),1):\n",
    "        sample = int(np.random.randint(0,len(small),1)[0])\n",
    "        store.append((big[j],small[sample]))\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c06725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiMentClf(torch.nn.Module):\n",
    "    def __init__(self,base_config,num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.base_config = base_config\n",
    "        self.dropout = torch.nn.Dropout(self.base_config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(self.base_config.hidden_size,self.num_labels)\n",
    "        self._init_weights(self.classifier)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "    \n",
    "    \n",
    "    def forward(self,data):\n",
    "        clf_out = self.classifier(self.dropout(data))\n",
    "        return clf_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_checkpoint,output_hidden_states=False,\\\n",
    "                                                    add_pooling_layer=False)\n",
    "        self.pooler = BertPooler(self.base_model.config)\n",
    "        self._init_weights(self.pooler)\n",
    "        self.task_classifier = SentiMentClf(self.base_model.config,2)\n",
    "        self.domain_classifier = SentiMentClf(self.base_model.config,2)\n",
    "        \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "        \n",
    "\n",
    "    def forward(self,task_data,domain_data=False,train_mode=True):\n",
    "        if train_mode:\n",
    "            out = self.base_model(input_ids=task_data['input_ids'], \\\n",
    "                               attention_mask=task_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            task_out = self.task_classifier(out)\n",
    "            \n",
    "            out = self.base_model(input_ids=domain_data['input_ids'], \\\n",
    "                               attention_mask=domain_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            domain_out = self.domain_classifier(out)\n",
    "            return task_out,domain_out\n",
    "        else:\n",
    "            out = self.base_model(input_ids=task_data['input_ids'], \\\n",
    "                               attention_mask=task_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            task_out = self.task_classifier(out)\n",
    "            return task_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830fa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8800bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "        seed = np.random.randint(0,1000000)\n",
    "        fix_all_seeds(seed)\n",
    "\n",
    "        dataset = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/review_labels.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "        dataset_src = dataset.train_test_split(0.2,shuffle=True)\n",
    "\n",
    "        dataset = load_dataset('csv',delimiter=\"\\t\",data_files='books/review_labels.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "        dataset_trg = dataset.train_test_split(0.2,shuffle=True)\n",
    "\n",
    "\n",
    "        dataset_domain = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/multitask_books_n_electronics_data.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset_domain['train']])\n",
    "        dataset_domain = dataset.train_test_split(0.1,shuffle=True)\n",
    "        \n",
    "        processed_datasets_src = dataset_src.map(preprocess_function,batched=True,\\\n",
    "                                      desc=\"Running tokenizer on dataset\",)\n",
    "\n",
    "        processed_datasets_trg = dataset_trg.map(preprocess_function,batched=True,\\\n",
    "                                  desc=\"Running tokenizer on dataset\",)\n",
    "        \n",
    "        processed_domain = dataset_domain.map(preprocess_function,batched=True,\\\n",
    "                                  desc=\"Running tokenizer on dataset\",)\n",
    "        \n",
    "        \n",
    "\n",
    "        processed_datasets_src.remove_columns_([\"text\"])\n",
    "        processed_datasets_trg.remove_columns_([\"text\"])\n",
    "        processed_domain.remove_columns_([\"text\"])\n",
    "        \n",
    "        \n",
    "        train_dataloader_src =DataLoader(processed_datasets_src['train'],\\\n",
    "                                             collate_fn=default_data_collator,\\\n",
    "                                             batch_size = 1,drop_last=True)\n",
    "        eval_dataloader_src =DataLoader(processed_datasets_src['test'],\\\n",
    "                                             collate_fn=default_data_collator,\\\n",
    "                                             batch_size = 16,drop_last=True)\n",
    "        \n",
    "        test_dataloader_trg =DataLoader(processed_datasets_trg['test'],\\\n",
    "                                             collate_fn=default_data_collator,\\\n",
    "                                             batch_size = 16,drop_last=True)\n",
    "        \n",
    "        train_dataloader_domain = DataLoader(processed_domain['test'],\\\n",
    "                                             collate_fn=default_data_collator,\\\n",
    "                                             batch_size = 1,drop_last=True)\n",
    "        \n",
    "        train_data = create_data_list(train_dataloader_src,train_dataloader_domain)\n",
    "        print(len(train_dataloader_src),len(train_dataloader_domain))\n",
    "        train_final = CombinedDataset(train_data)\n",
    "        final_train_loader = DataLoader(train_final, batch_size=16,\\\n",
    "                                            shuffle=True,drop_last=True)\n",
    "        \n",
    "        \n",
    "        return final_train_loader,eval_dataloader_src,test_dataloader_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18532493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(train_loader,eval_loader):\n",
    "            seed = np.random.randint(0,100)\n",
    "            fix_all_seeds(seed)\n",
    "            model = DANN()\n",
    "            model.cuda()\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            param_all1 = {}\n",
    "            param_all2 = {}\n",
    "            param_all3 = {}\n",
    "            for n,p in model.named_parameters():\n",
    "                if 'domain_classifier' not in n and 'task_classifier' not in n:\n",
    "                    param_all1[n]=p\n",
    "            \n",
    "            for n,p in model.named_parameters():\n",
    "                if 'task_classifier' in n:\n",
    "                    param_all2[n]=p\n",
    "            \n",
    "            for n,p in model.named_parameters():\n",
    "                if 'domain_classifier' in n:\n",
    "                    param_all3[n]=p\n",
    "                    \n",
    "                    \n",
    "            optimizer_grouped_transformer = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in param_all1.items() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in param_all1.items() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "                    \n",
    "            optimizer_grouped_task = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in param_all2.items() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in param_all2.items() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "            \n",
    "            \n",
    "            optimizer_grouped_domain = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in param_all3.items() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in param_all3.items() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "            optimizer_encoder = AdamW(optimizer_grouped_transformer, lr=5e-5)\n",
    "            optimizer_task = AdamW(optimizer_grouped_task, lr=5e-5)\n",
    "            optimizer_domain = AdamW(optimizer_grouped_domain, lr=5e-5)\n",
    "            fct_loss = CrossEntropyLoss()\n",
    "            scheduler1 = ExponentialLR(optimizer=optimizer_encoder,gamma=0.9,last_epoch=-1,verbose=True)\n",
    "            scheduler2 = ExponentialLR(optimizer=optimizer_task,gamma=0.9,last_epoch=-1,verbose=True)\n",
    "            scheduler3 = ExponentialLR(optimizer=optimizer_domain,gamma=0.9,last_epoch=-1,verbose=True)\n",
    "            \n",
    "            best_f1 = -1\n",
    "            best_loss = 1e5\n",
    "            for epoch in range(4):\n",
    "                print(f'EPOCH NO: {epoch}')\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                token_predictions_store = []\n",
    "                token_gold_store = []\n",
    "                for step, batch in enumerate(eval_loader):\n",
    "                    with torch.no_grad():\n",
    "                        input1 = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                                  'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                                     'labels':batch['labels'].cuda()}\n",
    "                        out  = model(input1,False,False)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(input1['labels'])\n",
    "                        loss = fct_loss(out,input1['labels'])\n",
    "                        val_loss = val_loss + loss.item()\n",
    "\n",
    "                predictions = torch.vstack(token_predictions_store)\n",
    "                references = torch.hstack(token_gold_store)\n",
    "                predictions = torch.argmax(predictions,axis=1)\n",
    "                print(predictions.shape,references.shape)\n",
    "                y_pred = predictions.detach().cpu().clone().numpy()\n",
    "                y_true = references.detach().cpu().clone().numpy()\n",
    "                eval_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "                print('-'*100)\n",
    "                print(eval_f1)\n",
    "                print(f'Epoch {epoch} validation loss {val_loss/len(eval_loader)}')\n",
    "                if eval_f1['f1'] > best_f1:\n",
    "                    best_f1 = eval_f1['f1']\n",
    "                    best_loss = val_loss/len(eval_loader)\n",
    "                    torch.save(model.state_dict(),\"saved_model/dann_amazon.bin\")\n",
    "                print('-'*100)\n",
    "        \n",
    "                model.train()\n",
    "                epoch_loss = 0.0\n",
    "                domain_epoch_loss = 0.0\n",
    "                for step, batch in enumerate(train_loader):\n",
    "                    input1 = get_data_slice(batch[1])\n",
    "                    input2 = get_data_slice(batch[0])\n",
    "                    out1,out2 = model(input1,input2,True)\n",
    "                    task_loss = fct_loss(out1,input1['labels'])\n",
    "                    domain_loss = fct_loss(out2,input2['labels'])\n",
    "                    epoch_loss = epoch_loss + task_loss.item()\n",
    "                    domain_epoch_loss = domain_epoch_loss + domain_loss.item()\n",
    "                    task_loss.backward()\n",
    "                    optimizer_encoder.step()\n",
    "                    optimizer_task.step()\n",
    "                    optimizer_encoder.zero_grad()\n",
    "                    optimizer_task.zero_grad()\n",
    "                    domain_loss.backward()\n",
    "                    for param in optimizer_encoder.param_groups[0]['params']:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad.data.mul_(-1)\n",
    "                    for param in optimizer_encoder.param_groups[1]['params']:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad.data.mul_(-1)\n",
    "                    optimizer_encoder.step()\n",
    "                    optimizer_domain.step()\n",
    "                    optimizer_encoder.zero_grad()\n",
    "                    optimizer_domain.zero_grad()\n",
    "                    \n",
    "                scheduler1.step()\n",
    "                scheduler2.step()\n",
    "                scheduler3.step()\n",
    "                print(f'Epoch {epoch} training task loss {epoch_loss/len(train_loader)}')\n",
    "                print(f'Epoch {epoch} training domain loss {domain_epoch_loss/len(train_loader)}')\n",
    "                print('**************************************************************************')\n",
    "            print(f'Best eval F1, loss {best_f1},{best_loss}')\n",
    "            return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1441e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(data):\n",
    "        model = DANN()\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(\"saved_model/dann_amazon.bin\"))\n",
    "        model.eval()\n",
    "        token_predictions_store = []\n",
    "        token_gold_store = []\n",
    "        for step, batch in enumerate(data):\n",
    "                    with torch.no_grad():\n",
    "                        input1 = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                                  'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                                     'labels':batch['labels'].cuda()}\n",
    "                        out  = model(input1,False,False)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(input1['labels'])\n",
    "\n",
    "        predictions = torch.vstack(token_predictions_store)\n",
    "        references = torch.hstack(token_gold_store)\n",
    "        predictions = torch.argmax(predictions,axis=1)\n",
    "        print(predictions.shape,references.shape)\n",
    "        y_pred = predictions.detach().cpu().clone().numpy()\n",
    "        y_true = references.detach().cpu().clone().numpy()\n",
    "        test_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "        print(f'Test F1 score {test_f1}')\n",
    "        return test_f1['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8667dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_loader, eval_loader, test_loader = prepare_data()\n",
    "    seed = run_train(train_loader,eval_loader)\n",
    "    return (seed,run_test(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddfe2630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ccd00502304857b3f0e72342d45fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b23a3eb08e4b52b93d8c7301be73cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10a799e5a3781c02\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d329ce6af248eab79505c453926ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ab8c0535d3459a9088a36b99f17683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37193fddc3594e719fd5acfcc7da33bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f834558e3449619646d09c8374734f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dec28bba9e5470590177ee46ee64800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecb6054a975431aa1297f3d32e60816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e38eccfef74cc396bcea45990a8159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-2c45b69591ba>:29: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use DatasetDict.remove_columns instead.\n",
      "  processed_datasets_src.remove_columns_([\"text\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.3184713375796178}\n",
      "Epoch 0 validation loss 0.7032849550247192\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Epoch 0 training task loss 0.5823406065694131\n",
      "Epoch 0 training domain loss 5.855141483325161\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8047808764940237}\n",
      "Epoch 1 validation loss 0.49246096014976504\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Epoch 1 training task loss 0.6343953980957174\n",
      "Epoch 1 training domain loss 1.3812580478315986\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.7075928917609047}\n",
      "Epoch 2 validation loss 0.7167817771434783\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Epoch 2 training task loss 0.7093283718886692\n",
      "Epoch 2 training domain loss 0.7942808183228097\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.0}\n",
      "Epoch 3 validation loss 0.6952649188041687\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Epoch 3 training task loss 0.7075112792257648\n",
      "Epoch 3 training domain loss 0.7117136683613581\n",
      "**************************************************************************\n",
      "Best eval F1, loss 0.8047808764940237,0.49246096014976504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.6619469026548673}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d612166dd0b4bdd87e19d53bb57c36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265f3e9824b14a75b59d5910b6b543a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10a799e5a3781c02\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d94939aed64c38b2c8a23bc486e5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddb29ca5a344070b04ba43ed3debef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a07839b9f1c48eba96fa96930f39bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b36d44593d480aa011f3fbca3ac207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03294919a8764e58b79bd14a3c61e9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870cc879d08246258c6901465853e99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0353ebe547c24898b4ed91f775080d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6424870466321244}\n",
      "Epoch 0 validation loss 0.7027775502204895\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Epoch 0 training task loss 0.4943171250358068\n",
      "Epoch 0 training domain loss 6.196136285203674\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.020202020202020204}\n",
      "Epoch 1 validation loss 0.7358540558815002\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Epoch 1 training task loss 0.60339833510462\n",
      "Epoch 1 training domain loss 1.2860793128661578\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8934010152284263}\n",
      "Epoch 2 validation loss 0.25420534059405325\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Epoch 2 training task loss 0.10543863791388294\n",
      "Epoch 2 training domain loss 0.9603193701351976\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8906249999999999}\n",
      "Epoch 3 validation loss 0.33580957893282176\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Epoch 3 training task loss 0.03351909767687061\n",
      "Epoch 3 training domain loss 0.7832656724943101\n",
      "**************************************************************************\n",
      "Best eval F1, loss 0.8934010152284263,0.25420534059405325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.66890756302521}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69a2c46b064457086c10433f42b92a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f8a12cbe5d4ed3907fcf66a5eb645b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10a799e5a3781c02\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142326568b74483296c4d37d99a50fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7b67ed531a4544a8c0a935fa3cceaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8b7b322e37434cb1d3bc644ca26068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c32e1fa80844189565b85c0e466964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14c56c2130f4bc581931b1ff5077d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4e6341e127483e87d517a4335a3091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2063ee43373a4d828568c7248e702732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.24908424908424906}\n",
      "Epoch 0 validation loss 0.6957817482948303\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Adjusting learning rate of group 0 to 4.5000e-05.\n",
      "Adjusting learning rate of group 1 to 4.5000e-05.\n",
      "Epoch 0 training task loss 0.559277503955655\n",
      "Epoch 0 training domain loss 7.641663565660603\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8194842406876791}\n",
      "Epoch 1 validation loss 0.3893849730491638\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Adjusting learning rate of group 0 to 4.0500e-05.\n",
      "Adjusting learning rate of group 1 to 4.0500e-05.\n",
      "Epoch 1 training task loss 0.10671529600085229\n",
      "Epoch 1 training domain loss 1.9494978550834523\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8501362397820164}\n",
      "Epoch 2 validation loss 0.5224260567128658\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Adjusting learning rate of group 0 to 3.6450e-05.\n",
      "Adjusting learning rate of group 1 to 3.6450e-05.\n",
      "Epoch 2 training task loss 0.050752021802689895\n",
      "Epoch 2 training domain loss 0.9472567127556751\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8548812664907652}\n",
      "Epoch 3 validation loss 0.4323249711841345\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Adjusting learning rate of group 0 to 3.2805e-05.\n",
      "Adjusting learning rate of group 1 to 3.2805e-05.\n",
      "Epoch 3 training task loss 0.040559810504539565\n",
      "Epoch 3 training domain loss 0.8050622406321536\n",
      "**************************************************************************\n",
      "Best eval F1, loss 0.8548812664907652,0.4323249711841345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.6688963210702341}\n",
      "0.6665835955834372 0.003278640224318106\n"
     ]
    }
   ],
   "source": [
    "test_best = {}\n",
    "for i in range(3):\n",
    "    out = main()\n",
    "    test_best[out[0]] = out[1]\n",
    "print(np.mean(list(test_best.values())),np.std(list(test_best.values())))\n",
    "pickle.dump(test_best,open(\"trg_books_dann.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc8b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
