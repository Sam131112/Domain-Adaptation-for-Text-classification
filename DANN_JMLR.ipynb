{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de661b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from collections import Counter\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 16\n",
    "from transformers import AutoConfig,AutoModel\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer,DataCollatorWithPadding,\\\n",
    "                                        TrainingArguments, Trainer,default_data_collator,AdamW\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)\n",
    "from torch.utils.data import DataLoader,RandomSampler\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "from datasets import load_dataset,load_metric\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "from torch.autograd import Function\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "pr_metric = load_metric('precision')\n",
    "re_metric = load_metric('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1344956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.utils.data.dataset import ConcatDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c1e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,comb_data):\n",
    "        self.data = comb_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_data,mlm_data = self.data[idx][0],self.data[idx][1]\n",
    "        return token_data, mlm_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5024356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1569e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_slice(i):\n",
    "                    inputs = {\n",
    "                   'input_ids':i['input_ids'].squeeze(1).cuda(),\\\n",
    "                   'attention_mask':i['attention_mask'].squeeze(1).cuda(),\\\n",
    "                   'labels':i['labels'].squeeze(1).cuda(),\n",
    "                       }\n",
    "                    return inputs \n",
    "\n",
    "def preprocess_function(examples):\n",
    "        result = tokenizer(examples['text'], padding=\"max_length\", max_length=512, truncation=True)\n",
    "        return result\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def create_data_list(data1,data2):\n",
    "    data1 = [batch for step,batch in enumerate(data1)]\n",
    "    data2 = [batch for step,batch in enumerate(data2)]\n",
    "    store = []\n",
    "    if min(len(data1),len(data2))==len(data1):\n",
    "        small = data1\n",
    "        big = data2\n",
    "    else:\n",
    "        small = data2\n",
    "        big = data1\n",
    "    for i in range(len(small)):\n",
    "        store.append((big[i],small[i]))\n",
    "    for j in range(i,len(big),1):\n",
    "        sample = int(np.random.randint(0,len(small),1)[0])\n",
    "        store.append((big[j],small[sample]))\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c012f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c06725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiMentClf(torch.nn.Module):\n",
    "    def __init__(self,base_config,num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.base_config = base_config\n",
    "        self.dropout = torch.nn.Dropout(self.base_config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(self.base_config.hidden_size,self.num_labels)\n",
    "        self._init_weights(self.classifier)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "    \n",
    "    \n",
    "    def forward(self,data):\n",
    "        clf_out = self.classifier(self.dropout(data))\n",
    "        return clf_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DANN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_checkpoint,output_hidden_states=False,\\\n",
    "                                                    add_pooling_layer=False)\n",
    "        self.pooler = BertPooler(self.base_model.config)\n",
    "        self._init_weights(self.pooler)\n",
    "        self.task_classifier = SentiMentClf(self.base_model.config,2)\n",
    "        self.domain_classifier = SentiMentClf(self.base_model.config,2)\n",
    "        \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "        \n",
    "\n",
    "    def forward(self,task_data,domain_data=False,train_mode=True):\n",
    "        if train_mode:\n",
    "            out = self.base_model(input_ids=task_data['input_ids'], \\\n",
    "                               attention_mask=task_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            task_out = self.task_classifier(out)\n",
    "            \n",
    "            out = self.base_model(input_ids=domain_data['input_ids'], \\\n",
    "                               attention_mask=domain_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            domain_out = self.domain_classifier(out)\n",
    "            return task_out,domain_out\n",
    "        else:\n",
    "            out = self.base_model(input_ids=task_data['input_ids'], \\\n",
    "                               attention_mask=task_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            task_out = self.task_classifier(out)\n",
    "            return task_out\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class DANNauto(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_checkpoint,output_hidden_states=False,\\\n",
    "                                                    add_pooling_layer=False)\n",
    "        self.pooler = BertPooler(self.base_model.config)\n",
    "        self._init_weights(self.pooler)\n",
    "        self.task_classifier = SentiMentClf(self.base_model.config,2)\n",
    "        self.domain_classifier = SentiMentClf(self.base_model.config,2)\n",
    "        self.my_grad = ReverseLayerF.apply\n",
    "        \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "        \n",
    "\n",
    "    def forward(self,task_data,domain_data=False,train_mode=True):\n",
    "        if train_mode:\n",
    "            out = self.base_model(input_ids=task_data['input_ids'], \\\n",
    "                               attention_mask=task_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            task_out = self.task_classifier(out)\n",
    "            \n",
    "            out = self.base_model(input_ids=domain_data['input_ids'], \\\n",
    "                               attention_mask=domain_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            domain_out = self.domain_classifier(self.my_grad(out,-1))\n",
    "            return task_out,domain_out\n",
    "        else:\n",
    "            out = self.base_model(input_ids=task_data['input_ids'], \\\n",
    "                               attention_mask=task_data['attention_mask'])           \n",
    "            out = self.pooler(out.last_hidden_state)\n",
    "            task_out = self.task_classifier(out)\n",
    "            return task_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830fa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8800bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "        seed = np.random.randint(0,1000000)\n",
    "        fix_all_seeds(seed)\n",
    "\n",
    "        dataset = load_dataset('csv',delimiter=\"\\t\",data_files='books/review_labels.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "        dataset_src = dataset.train_test_split(0.2,shuffle=True)\n",
    "\n",
    "        dataset = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/review_labels.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "        dataset_trg = dataset.train_test_split(0.2,shuffle=True)\n",
    "\n",
    "\n",
    "        dataset_domain = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/multitask_books_n_electronics_data.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset_domain['train']])\n",
    "        dataset_domain = dataset.train_test_split(0.1,shuffle=True)\n",
    "        \n",
    "        processed_datasets_src = dataset_src.map(preprocess_function,batched=True,\\\n",
    "                                      desc=\"Running tokenizer on dataset\",)\n",
    "\n",
    "        processed_datasets_trg = dataset_trg.map(preprocess_function,batched=True,\\\n",
    "                                  desc=\"Running tokenizer on dataset\",)\n",
    "        \n",
    "        processed_domain = dataset_domain.map(preprocess_function,batched=True,\\\n",
    "                                  desc=\"Running tokenizer on dataset\",)\n",
    "        \n",
    "        \n",
    "\n",
    "        processed_datasets_src.remove_columns_([\"text\"])\n",
    "        processed_datasets_trg.remove_columns_([\"text\"])\n",
    "        processed_domain.remove_columns_([\"text\"])\n",
    "        \n",
    "        \n",
    "        train_dataloader_src =DataLoader(processed_datasets_src['train'],\\\n",
    "                                             collate_fn=default_data_collator,\\\n",
    "                                             batch_size = 1,drop_last=True)\n",
    "        eval_dataloader_src =DataLoader(processed_datasets_src['test'],\\\n",
    "                                             collate_fn=default_data_collator,\\\n",
    "                                             batch_size = 16,drop_last=True)\n",
    "        \n",
    "        test_dataloader_trg =DataLoader(processed_datasets_trg['test'],\\\n",
    "                                             collate_fn=default_data_collator,\\\n",
    "                                             batch_size = 16,drop_last=True)\n",
    "        \n",
    "        train_dataloader_domain = DataLoader(processed_domain['test'],\\\n",
    "                                             collate_fn=default_data_collator,\\\n",
    "                                             batch_size = 1,drop_last=True)\n",
    "        \n",
    "        train_data = create_data_list(train_dataloader_src,train_dataloader_domain)\n",
    "        print(len(train_dataloader_src),len(train_dataloader_domain))\n",
    "        train_final = CombinedDataset(train_data)\n",
    "        final_train_loader = DataLoader(train_final, batch_size=16,\\\n",
    "                                            shuffle=True,drop_last=True)\n",
    "        \n",
    "        \n",
    "        return final_train_loader,eval_dataloader_src,test_dataloader_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09c53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_a(train_loader,eval_loader):\n",
    "            seed = np.random.randint(0,100)\n",
    "            fix_all_seeds(seed)\n",
    "            model = DANN()\n",
    "            model.cuda()\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            param_all1 = {}\n",
    "            param_all2 = {}\n",
    "            param_all3 = {}\n",
    "            for n,p in model.named_parameters():\n",
    "                if 'domain_classifier' not in n and 'task_classifier' not in n:\n",
    "                    param_all1[n]=p\n",
    "            \n",
    "            for n,p in model.named_parameters():\n",
    "                if 'task_classifier' in n:\n",
    "                    param_all2[n]=p\n",
    "            \n",
    "            for n,p in model.named_parameters():\n",
    "                if 'domain_classifier' in n:\n",
    "                    param_all3[n]=p\n",
    "                    \n",
    "                    \n",
    "            optimizer_grouped_transformer = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in param_all1.items() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in param_all1.items() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "                    \n",
    "            optimizer_grouped_task = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in param_all2.items() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in param_all2.items() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "            \n",
    "            \n",
    "            optimizer_grouped_domain = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in param_all3.items() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in param_all3.items() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "            optimizer_encoder = AdamW(optimizer_grouped_transformer, lr=5e-5)\n",
    "            optimizer_task = AdamW(optimizer_grouped_task, lr=5e-5)\n",
    "            optimizer_domain = AdamW(optimizer_grouped_domain, lr=5e-5)\n",
    "            fct_loss = CrossEntropyLoss()\n",
    "            scheduler1 = ExponentialLR(optimizer=optimizer_encoder,gamma=0.85,last_epoch=-1,verbose=True)\n",
    "            scheduler2 = ExponentialLR(optimizer=optimizer_task,gamma=0.85,last_epoch=-1,verbose=True)\n",
    "            scheduler3 = ExponentialLR(optimizer=optimizer_domain,gamma=0.85,last_epoch=-1,verbose=True)\n",
    "            \n",
    "            best_f1 = -1\n",
    "            best_loss = 1e5\n",
    "            for epoch in range(5):\n",
    "                print(f'EPOCH NO: {epoch}')\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                token_predictions_store = []\n",
    "                token_gold_store = []\n",
    "                for step, batch in enumerate(eval_loader):\n",
    "                    with torch.no_grad():\n",
    "                        input1 = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                                  'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                                     'labels':batch['labels'].cuda()}\n",
    "                        out  = model(input1,False,False)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(input1['labels'])\n",
    "                        loss = fct_loss(out,input1['labels'])\n",
    "                        val_loss = val_loss + loss.item()\n",
    "\n",
    "                predictions = torch.vstack(token_predictions_store)\n",
    "                references = torch.hstack(token_gold_store)\n",
    "                predictions = torch.argmax(predictions,axis=1)\n",
    "                print(predictions.shape,references.shape)\n",
    "                y_pred = predictions.detach().cpu().clone().numpy()\n",
    "                y_true = references.detach().cpu().clone().numpy()\n",
    "                eval_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "                print('-'*100)\n",
    "                print(eval_f1)\n",
    "                print(f'Epoch {epoch} validation loss {val_loss/len(eval_loader)}')\n",
    "                if eval_f1['f1'] > best_f1:\n",
    "                    best_f1 = eval_f1['f1']\n",
    "                    best_loss = val_loss/len(eval_loader)\n",
    "                    torch.save(model.state_dict(),\"saved_model/dann_amazon.bin\")\n",
    "                print('-'*100)\n",
    "        \n",
    "                model.train()\n",
    "                epoch_loss = 0.0\n",
    "                domain_epoch_loss = 0.0\n",
    "                for step, batch in enumerate(train_loader):\n",
    "                    input1 = get_data_slice(batch[1])\n",
    "                    input2 = get_data_slice(batch[0])\n",
    "                    out1,out2 = model(input1,input2,True)\n",
    "                    task_loss = fct_loss(out1,input1['labels'])\n",
    "                    domain_loss = fct_loss(out2,input2['labels'])\n",
    "                    epoch_loss = epoch_loss + task_loss.item()\n",
    "                    domain_epoch_loss = domain_epoch_loss + domain_loss.item()\n",
    "                    task_loss.backward()\n",
    "                    optimizer_encoder.step()\n",
    "                    optimizer_task.step()\n",
    "                    optimizer_encoder.zero_grad()\n",
    "                    optimizer_task.zero_grad()\n",
    "                    domain_loss.backward()\n",
    "                    for param in optimizer_encoder.param_groups[0]['params']:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad.data.mul_(-1)\n",
    "                    for param in optimizer_encoder.param_groups[1]['params']:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad.data.mul_(-1)\n",
    "                    optimizer_encoder.step()\n",
    "                    optimizer_domain.step()\n",
    "                    optimizer_encoder.zero_grad()\n",
    "                    optimizer_domain.zero_grad()\n",
    "                    \n",
    "                scheduler1.step()\n",
    "                scheduler2.step()\n",
    "                scheduler3.step()\n",
    "                print(f'Epoch {epoch} training task loss {epoch_loss/len(train_loader)}')\n",
    "                print(f'Epoch {epoch} training domain loss {domain_epoch_loss/len(train_loader)}')\n",
    "                print('**************************************************************************')\n",
    "            print(f'Best eval F1, loss {best_f1},{best_loss}')\n",
    "            return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9810b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_b(train_loader,eval_loader):\n",
    "            seed = np.random.randint(0,100)\n",
    "            fix_all_seeds(seed)\n",
    "            model = DANNauto()\n",
    "            model.cuda()\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in model.named_parameters() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in model.named_parameters() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "            optimizer = AdamW(optimizer_grouped, lr=5e-5)\n",
    "            fct_loss = CrossEntropyLoss()\n",
    "            scheduler = ExponentialLR(optimizer=optimizer,gamma=0.85,last_epoch=-1,verbose=True)\n",
    "            \n",
    "            best_f1 = -1\n",
    "            best_loss = 1e5\n",
    "            for epoch in range(5):\n",
    "                print(f'EPOCH NO: {epoch}')\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                token_predictions_store = []\n",
    "                token_gold_store = []\n",
    "                for step, batch in enumerate(eval_loader):\n",
    "                    with torch.no_grad():\n",
    "                        input1 = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                                  'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                                     'labels':batch['labels'].cuda()}\n",
    "                        out  = model(input1,False,False)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(input1['labels'])\n",
    "                        loss = fct_loss(out,input1['labels'])\n",
    "                        val_loss = val_loss + loss.item()\n",
    "\n",
    "                predictions = torch.vstack(token_predictions_store)\n",
    "                references = torch.hstack(token_gold_store)\n",
    "                predictions = torch.argmax(predictions,axis=1)\n",
    "                print(predictions.shape,references.shape)\n",
    "                y_pred = predictions.detach().cpu().clone().numpy()\n",
    "                y_true = references.detach().cpu().clone().numpy()\n",
    "                eval_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "                print('-'*100)\n",
    "                print(eval_f1)\n",
    "                print(f'Epoch {epoch} validation loss {val_loss/len(eval_loader)}')\n",
    "                if eval_f1['f1'] > best_f1:\n",
    "                    best_f1 = eval_f1['f1']\n",
    "                    best_loss = val_loss/len(eval_loader)\n",
    "                    torch.save(model.state_dict(),\"saved_model/dann_amazon.bin\")\n",
    "                print('-'*100)\n",
    "        \n",
    "                model.train()\n",
    "                epoch_loss = 0.0\n",
    "                domain_epoch_loss = 0.0\n",
    "                for step, batch in enumerate(train_loader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input1 = get_data_slice(batch[1])\n",
    "                    input2 = get_data_slice(batch[0])\n",
    "                    out1,out2 = model(input1,input2,True)\n",
    "                    task_loss = fct_loss(out1,input1['labels'])\n",
    "                    domain_loss = fct_loss(out2,input2['labels'])\n",
    "                    epoch_loss = epoch_loss + task_loss.item()\n",
    "                    domain_epoch_loss = domain_epoch_loss + domain_loss.item()\n",
    "                    loss = task_loss + domain_loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                print(f'Epoch {epoch} training task loss {epoch_loss/len(train_loader)}')\n",
    "                print(f'Epoch {epoch} training domain loss {domain_epoch_loss/len(train_loader)}')\n",
    "                print('**************************************************************************')\n",
    "            print(f'Best eval F1, loss {best_f1},{best_loss}')\n",
    "            return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2712bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(data):\n",
    "        model = DANNauto()\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(\"saved_model/dann_amazon.bin\"))\n",
    "        model.eval()\n",
    "        token_predictions_store = []\n",
    "        token_gold_store = []\n",
    "        for step, batch in enumerate(data):\n",
    "                    with torch.no_grad():\n",
    "                        input1 = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                                  'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                                     'labels':batch['labels'].cuda()}\n",
    "                        out  = model(input1,False,False)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(input1['labels'])\n",
    "\n",
    "        predictions = torch.vstack(token_predictions_store)\n",
    "        references = torch.hstack(token_gold_store)\n",
    "        predictions = torch.argmax(predictions,axis=1)\n",
    "        print(predictions.shape,references.shape)\n",
    "        y_pred = predictions.detach().cpu().clone().numpy()\n",
    "        y_true = references.detach().cpu().clone().numpy()\n",
    "        test_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "        print(f'Test F1 score {test_f1}')\n",
    "        return test_f1['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "520dc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_loader, eval_loader, test_loader = prepare_data()\n",
    "    seed = run_train_b(train_loader,eval_loader)\n",
    "    return (seed,run_test(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4fe16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56879fcf44714ab0aaaa3c850c169fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1159a51a242e4073a45fc3eee7429f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10a799e5a3781c02\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7f700a916941dd8e23744e04a5b2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1360b071304d87a78b93102fd3ccdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10aa6d0d1989475a892e77c6eae7c684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf4f1923ee14541960be33de97d0522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d24618c236d4c6591006e2defb30ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b65f51766e45c3b158d93b86a681f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468fbaeb2e2548758f5a4218b4c81f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6151142355008788}\n",
      "Epoch 0 validation loss 0.7022389149665833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.2500e-05.\n",
      "Adjusting learning rate of group 1 to 4.2500e-05.\n",
      "Epoch 0 training task loss 0.19417803612365694\n",
      "Epoch 0 training domain loss 0.06165424108388428\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9095477386934674}\n",
      "Epoch 1 validation loss 0.28002619951963426\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.6125e-05.\n",
      "Adjusting learning rate of group 1 to 3.6125e-05.\n",
      "Epoch 1 training task loss 0.03195394327594714\n",
      "Epoch 1 training domain loss 0.009703791646152177\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8820224719101124}\n",
      "Epoch 2 validation loss 0.4735213242471218\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.0706e-05.\n",
      "Adjusting learning rate of group 1 to 3.0706e-05.\n",
      "Epoch 2 training task loss 0.010933140308076178\n",
      "Epoch 2 training domain loss 0.005095612026206688\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9132653061224489}\n",
      "Epoch 3 validation loss 0.39919603840098716\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.6100e-05.\n",
      "Adjusting learning rate of group 1 to 2.6100e-05.\n",
      "Epoch 3 training task loss 0.011533648905122023\n",
      "Epoch 3 training domain loss 0.0007637463897056569\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8958333333333333}\n",
      "Epoch 4 validation loss 0.39579938708571716\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.2185e-05.\n",
      "Adjusting learning rate of group 1 to 2.2185e-05.\n",
      "Epoch 4 training task loss 0.00481365438288472\n",
      "Epoch 4 training domain loss 0.0002834096418655938\n",
      "**************************************************************************\n",
      "Best eval F1, loss 0.9132653061224489,0.39919603840098716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8020833333333334}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2103679be8f74665a7f23db1a08794e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41aeea086a94e5787e6d6bedddef8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10a799e5a3781c02\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2f56e78a8e4e8cb259b159e77d7da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edabe271588417b9b3577ac3e05ec8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91b375bc1984e29921f05fce8b7a240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4160981442d347b88e454e28ba66f64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d0a98c4f654668a4258c15692e937e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9de7d0897e54b32b15f329848b463b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33519b501694b1a913bb81d0a2c0da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.55859375}\n",
      "Epoch 0 validation loss 0.7013635206222534\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.2500e-05.\n",
      "Adjusting learning rate of group 1 to 4.2500e-05.\n",
      "Epoch 0 training task loss 0.24433051053927854\n",
      "Epoch 0 training domain loss 0.05787869359655403\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.901098901098901}\n",
      "Epoch 1 validation loss 0.23547301515936853\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.6125e-05.\n",
      "Adjusting learning rate of group 1 to 3.6125e-05.\n",
      "Epoch 1 training task loss 0.02666580393031817\n",
      "Epoch 1 training domain loss 0.012558443524926667\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9234828496042217}\n",
      "Epoch 2 validation loss 0.29549383074510843\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.0706e-05.\n",
      "Adjusting learning rate of group 1 to 3.0706e-05.\n",
      "Epoch 2 training task loss 0.021148156153625496\n",
      "Epoch 2 training domain loss 0.006716474330891167\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9193548387096774}\n",
      "Epoch 3 validation loss 0.2720452502509579\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.6100e-05.\n",
      "Adjusting learning rate of group 1 to 2.6100e-05.\n",
      "Epoch 3 training task loss 0.0014224224067771749\n",
      "Epoch 3 training domain loss 0.0007446045961043593\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9157894736842105}\n",
      "Epoch 4 validation loss 0.3441939053183887\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.2185e-05.\n",
      "Adjusting learning rate of group 1 to 2.2185e-05.\n",
      "Epoch 4 training task loss 0.0007796814133419626\n",
      "Epoch 4 training domain loss 0.0003605392185891232\n",
      "**************************************************************************\n",
      "Best eval F1, loss 0.9234828496042217,0.29549383074510843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8089887640449438}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dab54610fa8455cbddb3e69b331adf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f766075dc2c483ba659860a3049718f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10a799e5a3781c02\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba320858b1874136994eb481a6a3d036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbb6a093a8b407aa84416bc7beb547e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589897be300d48c09d939af0ffda8818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2120df65954dcd83fa20547fda5f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e8e25a97374edbaa0b501680e4faa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5728fbbb84e6468ab26d8cfb9e438a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3615806fda984497831ed2c927fef600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.5522041763341068}\n",
      "Epoch 0 validation loss 0.6932196187973022\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.2500e-05.\n",
      "Adjusting learning rate of group 1 to 4.2500e-05.\n",
      "Epoch 0 training task loss 0.18410063003845356\n",
      "Epoch 0 training domain loss 0.054609862552916805\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9073170731707317}\n",
      "Epoch 1 validation loss 0.29006324894726276\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.6125e-05.\n",
      "Adjusting learning rate of group 1 to 3.6125e-05.\n",
      "Epoch 1 training task loss 0.04382697329937581\n",
      "Epoch 1 training domain loss 0.01447928959079077\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9237875288683602}\n",
      "Epoch 2 validation loss 0.3147553166933358\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.0706e-05.\n",
      "Adjusting learning rate of group 1 to 3.0706e-05.\n",
      "Epoch 2 training task loss 0.009345204778095117\n",
      "Epoch 2 training domain loss 0.005585861563342234\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9019607843137255}\n",
      "Epoch 3 validation loss 0.3236585507541895\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.6100e-05.\n",
      "Adjusting learning rate of group 1 to 2.6100e-05.\n",
      "Epoch 3 training task loss 0.010035530252364717\n",
      "Epoch 3 training domain loss 0.0028837993388138843\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9160671462829737}\n",
      "Epoch 4 validation loss 0.2943981498852372\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.2185e-05.\n",
      "Adjusting learning rate of group 1 to 2.2185e-05.\n",
      "Epoch 4 training task loss 0.0019336859085649829\n",
      "Epoch 4 training domain loss 0.00047529679298948885\n",
      "**************************************************************************\n",
      "Best eval F1, loss 0.9237875288683602,0.3147553166933358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.7102473498233215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30793ba71284e44a111e15d0939425a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a53c0419e75473eb31ecab27220b11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10a799e5a3781c02\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9263931520f41508a0af1e819929e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5debbe054944ea899040a698e08a511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eec2c69babc47e885f3212c94469854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b898a06a184b4e30832f4458632c2c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846ce752e90347b784bbf7f04d758199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ecb6c32ee94b11b4b504cae5eb3162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf0228f43714f63b99ebe9a0ee665b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.11607142857142855}\n",
      "Epoch 0 validation loss 0.7027045893669128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.2500e-05.\n",
      "Adjusting learning rate of group 1 to 4.2500e-05.\n",
      "Epoch 0 training task loss 0.2082286083556465\n",
      "Epoch 0 training domain loss 0.06250153670252953\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8770053475935828}\n",
      "Epoch 1 validation loss 0.39221978599205615\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.6125e-05.\n",
      "Adjusting learning rate of group 1 to 3.6125e-05.\n",
      "Epoch 1 training task loss 0.040971515613822236\n",
      "Epoch 1 training domain loss 0.013620273095112658\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9126213592233009}\n",
      "Epoch 2 validation loss 0.2784971035644412\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.0706e-05.\n",
      "Adjusting learning rate of group 1 to 3.0706e-05.\n",
      "Epoch 2 training task loss 0.011802131565651151\n",
      "Epoch 2 training domain loss 0.007390673117818009\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9}\n",
      "Epoch 3 validation loss 0.4067198756488506\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.6100e-05.\n",
      "Adjusting learning rate of group 1 to 2.6100e-05.\n",
      "Epoch 3 training task loss 0.0006395146904394797\n",
      "Epoch 3 training domain loss 0.0005051433324237039\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9349999999999999}\n",
      "Epoch 4 validation loss 0.34911527209798804\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.2185e-05.\n",
      "Adjusting learning rate of group 1 to 2.2185e-05.\n",
      "Epoch 4 training task loss 0.00033418900666568583\n",
      "Epoch 4 training domain loss 0.0002731894410285492\n",
      "**************************************************************************\n",
      "Best eval F1, loss 0.9349999999999999,0.34911527209798804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.6644736842105262}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a063c027d64b31a84dc968803609dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-e969910f71f0b5af.arrow and /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-ad0c71debe5e73c1.arrow\n",
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ff1a9001f449ef85d9bfec6f626fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-5cccf6915c7a2606.arrow and /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-980aaa2df962b79c.arrow\n",
      "Using custom data configuration default-10a799e5a3781c02\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6935de2a15949808fad9cf21ab8d3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-807bdd271aa419fc.arrow and /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-9a7cb99905fb6357.arrow\n",
      "Loading cached processed dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-1a95ca2785cfe925.arrow\n",
      "Loading cached processed dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-c2a68324614674cf.arrow\n",
      "Loading cached processed dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-f66c575c9f94898d.arrow\n",
      "Loading cached processed dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-07c31801a5179524.arrow\n",
      "Loading cached processed dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-c15749c8041c6cf0.arrow\n",
      "Loading cached processed dataset at /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-10a799e5a3781c02/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-d37d092ac2a8ea3c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.69281045751634}\n",
      "Epoch 0 validation loss 0.6971366667747497\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.2500e-05.\n",
      "Adjusting learning rate of group 1 to 4.2500e-05.\n",
      "Epoch 0 training task loss 0.2005313458975781\n",
      "Epoch 0 training domain loss 0.04868249930254791\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9054945054945055}\n",
      "Epoch 1 validation loss 0.31150734826922416\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.6125e-05.\n",
      "Adjusting learning rate of group 1 to 3.6125e-05.\n",
      "Epoch 1 training task loss 0.033064518426666654\n",
      "Epoch 1 training domain loss 0.009464238448744276\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9107551487414188}\n",
      "Epoch 2 validation loss 0.3168096076510847\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.0706e-05.\n",
      "Adjusting learning rate of group 1 to 3.0706e-05.\n",
      "Epoch 2 training task loss 0.011481168755948323\n",
      "Epoch 2 training domain loss 0.0010120712537768715\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9133489461358314}\n",
      "Epoch 3 validation loss 0.3355873409844935\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.6100e-05.\n",
      "Adjusting learning rate of group 1 to 2.6100e-05.\n",
      "Epoch 3 training task loss 0.005756087738185803\n",
      "Epoch 3 training domain loss 0.0017738926570157955\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.89}\n",
      "Epoch 4 validation loss 0.5037252883054316\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 2.2185e-05.\n",
      "Adjusting learning rate of group 1 to 2.2185e-05.\n",
      "Epoch 4 training task loss 0.0026963822024315435\n",
      "Epoch 4 training domain loss 0.00037839383588927775\n",
      "**************************************************************************\n",
      "Best eval F1, loss 0.9133489461358314,0.3355873409844935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8342245989304813}\n",
      "0.7640035460685212 0.06511629340769602\n"
     ]
    }
   ],
   "source": [
    "test_best = {}\n",
    "for i in range(5):\n",
    "    out = main()\n",
    "    test_best[out[0]] = out[1]\n",
    "print(np.mean(list(test_best.values())),np.std(list(test_best.values())))\n",
    "pickle.dump(test_best,open(\"trg_books_dann.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6bf05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
