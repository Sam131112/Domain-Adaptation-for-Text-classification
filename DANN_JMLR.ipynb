{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de661b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import math\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "batch_size = 16\n",
    "from transformers import AutoConfig,AutoModel\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer,DataCollatorWithPadding,\\\n",
    "                                        TrainingArguments, Trainer,default_data_collator,AdamW\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)\n",
    "from torch.utils.data import DataLoader,RandomSampler\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "from datasets import load_dataset,load_metric\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "f1_metric = load_metric(\"f1\")\n",
    "pr_metric = load_metric('precision')\n",
    "re_metric = load_metric('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5024356",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1569e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_slice(i):\n",
    "                    inputs = {\n",
    "                   'input_ids':i['input_ids'].cuda(),\\\n",
    "                   'attention_mask':i['attention_mask'].cuda(),\\\n",
    "                   'labels':i['labels'].cuda(),\n",
    "                       }\n",
    "                    return inputs \n",
    "\n",
    "def preprocess_function(examples):\n",
    "        result = tokenizer(examples['text'], padding=\"max_length\", max_length=512, truncation=True)\n",
    "        return result\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c06725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_checkpoint,output_hidden_states=False,\\\n",
    "                                                    add_pooling_layer=False)\n",
    "        self.pooler = BertPooler(self.base_model.config)\n",
    "        self._init_weights(self.pooler)\n",
    "        \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "        \n",
    "\n",
    "    def forward(self,data):\n",
    "        out = self.base_model(input_ids=data['input_ids'], \\\n",
    "                               attention_mask=data['attention_mask'])\n",
    "        out = self.pooler(out.last_hidden_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7830fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiMentDomain_Clf(torch.nn.Module):\n",
    "    def __init__(self,base_model,num_labels):\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = torch.nn.Dropout(self.base_model.config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(self.base_model.config.hidden_size,self.num_labels)\n",
    "        self._init_weights(self.classifier)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "    \n",
    "    \n",
    "    def forward(self,data):\n",
    "        clf_out = self.classifier(self.dropout(data))\n",
    "        return clf_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8800bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e38dfb0de6451c818f8b4c07ca841a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b47e9187f0f429d8248405f9d77e381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5ffc5ef371034bf6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-5ffc5ef371034bf6/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2b79f02a9649e0b5cd0b63edf12ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53457e2e02b04f9cb04b271d9f91f618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-5ffc5ef371034bf6/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e936ff62bde4edb9d43fdc169a0624c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = np.random.randint(0,1000000)\n",
    "fix_all_seeds(seed)\n",
    "\n",
    "dataset = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/review_labels.csv')\n",
    "dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "dataset_src = dataset.train_test_split(0.2,shuffle=False)\n",
    "\n",
    "dataset = load_dataset('csv',delimiter=\"\\t\",data_files='books/review_labels.csv')\n",
    "dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "dataset_trg = dataset.train_test_split(0.2,shuffle=False)\n",
    "\n",
    "\n",
    "dataset = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/multitask_books_n_electronics_data.csv')\n",
    "dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "dataset_domain = dataset.train_test_split(0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0053e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': \"The eVGA e-GeForce 7300 GS is just what I needed to play the type of games I play......My computer had intergrated video before and that just didn't cut it....With my 288 power supply it works great and was a snap to install...Also very good service from Amazon....\",\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_domain['train'][900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15515420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36808, 1600)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_domain['train']),len(dataset_trg['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4a3cf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2d88a8056d15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_src\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_src' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_src['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac890a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
