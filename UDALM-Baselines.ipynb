{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os\n",
    "import math\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "#model_checkpoint = \"allenai/scibert_scivocab_cased\"\n",
    "#model_checkpoint = \"roberta-large\"\n",
    "batch_size = 16\n",
    "from transformers import AutoConfig,AutoModel\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer,DataCollatorWithPadding,\\\n",
    "                                        TrainingArguments, Trainer,default_data_collator,AdamW,\\\n",
    "                                        BertModelWithHeads,PfeifferConfig,PfeifferInvConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)\n",
    "from torch.utils.data import DataLoader,RandomSampler\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "#tokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint,add_prefix_space=True)\n",
    "import transformers.adapters.composition as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_metric\n",
    "import datasets\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = load_metric(\"f1\")\n",
    "pr_metric = load_metric('precision')\n",
    "re_metric = load_metric('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedSequenceModel(torch.nn.Module):\n",
    "    def __init__(self,labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = labels\n",
    "        self.base_model = AutoModel.from_pretrained(model_checkpoint,output_hidden_states=False,add_pooling_layer=False)\n",
    "        #Pretrained Using MLM and saved \n",
    "        self.base_model.load_state_dict(torch.load(\"mlm_books/books_amazon.bin\"))\n",
    "        self.dropout = torch.nn.Dropout(self.base_model.config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(self.base_model.config.hidden_size,self.num_labels)\n",
    "        self._init_weights(self.classifier)\n",
    "        self.pooler = BertPooler(self.base_model.config)\n",
    "        self._init_weights(self.pooler)\n",
    "        \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "        \n",
    "\n",
    "    def forward(self,data):\n",
    "        out = self.base_model(input_ids=data['input_ids'], \\\n",
    "                               attention_mask=data['attention_mask'])\n",
    "        out = self.pooler(out.last_hidden_state)\n",
    "        clf_out = self.classifier(self.dropout(out))\n",
    "        return clf_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedAdapterSequenceModel(torch.nn.Module):\n",
    "    def __init__(self,labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = labels\n",
    "        self.task_config = PfeifferConfig()\n",
    "        self.base_model = BertModelWithHeads.from_pretrained(model_checkpoint)\n",
    "        #Pretrained Using MLM and saved \n",
    "        self.base_model.load_adapter(\"mlm_electronics/checkpoint-22071/mlm/\",set_active=False,overwrite_ok=True,with_head=False)\n",
    "        self.base_model.add_adapter(\"sentiment\",set_active=False,overwrite_ok=True,config=self.task_config)\n",
    "        self.base_model.add_classification_head(\"sentiment\",num_labels=self.num_labels,overwrite_ok=True)\n",
    "        #self.base_model.delete_head(\"mlm\")\n",
    "        self.base_model.set_active_adapters(ac.Stack(\"mlm\",\"sentiment\"))\n",
    "        self.base_model.train_adapter(['sentiment'])\n",
    "\n",
    "\n",
    "    def forward(self,data):\n",
    "        clf_out = self.base_model(input_ids=data['input_ids'], \\\n",
    "                               attention_mask=data['attention_mask'])\n",
    "        return clf_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "        result = tokenizer(examples['text'], padding=\"max_length\", max_length=512, truncation=True)\n",
    "        return result\n",
    "\n",
    "def compute_metrics(p):\n",
    "    f1_metric = load_metric(\"f1\")\n",
    "    pr_metric = load_metric('precision')\n",
    "    re_metric = load_metric('recall')\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    results1 = f1_metric.compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    results2 = pr_metric.compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    results3 = re_metric.compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    return {\n",
    "        \"precision\": results2[\"precision\"],\n",
    "        \"recall\": results3[\"recall\"],\n",
    "        \"f1\": results1[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63529"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline1():   \n",
    "            \n",
    "            seed = np.random.randint(0,1000000)\n",
    "            fix_all_seeds(seed)\n",
    "            dataset = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/review_labels.csv')\n",
    "            dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "            dataset_src = dataset.train_test_split(0.2,shuffle=False)\n",
    "            \n",
    "            dataset = load_dataset('csv',delimiter=\"\\t\",data_files='books/review_labels.csv')\n",
    "            dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "            dataset_trg = dataset.train_test_split(0.2,shuffle=False)\n",
    "            \n",
    "            processed_datasets_src = dataset_src.map(preprocess_function,batched=True,\\\n",
    "                                      desc=\"Running tokenizer on dataset\",)\n",
    "\n",
    "            processed_datasets_trg = dataset_trg.map(preprocess_function,batched=True,\\\n",
    "                                      desc=\"Running tokenizer on dataset\",)\n",
    "            \n",
    "            processed_datasets_src.remove_columns_([\"text\"])\n",
    "            processed_datasets_trg.remove_columns_([\"text\"])\n",
    "            \n",
    "            config = AutoConfig.from_pretrained(model_checkpoint,num_labels=2,)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,config=config,)\n",
    "            \n",
    "            args = TrainingArguments(\n",
    "                \"sanity-chunk\",\n",
    "                evaluation_strategy = \"epoch\",\n",
    "                learning_rate=5e-5,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                num_train_epochs=5,\n",
    "                weight_decay=0.01,\n",
    "                save_strategy=\"epoch\",\n",
    "                logging_steps=100,\n",
    "                overwrite_output_dir=True,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model = \"eval_f1\",\n",
    "                seed = seed,\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                    model,\n",
    "                    args,\n",
    "                    train_dataset=processed_datasets_src['train'],\n",
    "                    eval_dataset= processed_datasets_src['test'],\n",
    "                    data_collator=default_data_collator,\n",
    "                    tokenizer=tokenizer,\n",
    "                    compute_metrics=compute_metrics\n",
    "                )\n",
    "            \n",
    "            trainer.train(resume_from_checkpoint=None)\n",
    "            #p = trainer.predict(processed_datasets_src['test'])\n",
    "            p = trainer.predict(processed_datasets_trg['test'])\n",
    "            \n",
    "            y_hat = np.argmax(p.predictions,1)\n",
    "            y = p.label_ids\n",
    "            out = f1_metric.compute(predictions=y_hat,references=y)\n",
    "            return out['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(final_train_loader,final_eval_loader):\n",
    "            model = PretrainedAdapterSequenceModel(2)\n",
    "            model.cuda()\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped_parameters = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in model.named_parameters() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in model.named_parameters() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "            fct_loss = CrossEntropyLoss()\n",
    "            scheduler = ExponentialLR(optimizer=optimizer,gamma=0.99,last_epoch=-1,verbose=True)\n",
    "            \n",
    "            best_loss = 1e5\n",
    "            best_f1 = -1\n",
    "            for epoch in range(10):\n",
    "                print(f'EPOCH NO: {epoch}')\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                token_predictions_store = []\n",
    "                token_gold_store = []\n",
    "                for step, batch in enumerate(final_eval_loader):\n",
    "                    with torch.no_grad():\n",
    "                        data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                        out = model(data)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(batch['labels'])\n",
    "                        loss = fct(out.view(-1,model.num_labels),\\\n",
    "                                                   data['labels'].view(-1))\n",
    "                        val_loss = val_loss + loss.item()\n",
    "\n",
    "                predictions = torch.vstack(token_predictions_store)\n",
    "                references = torch.hstack(token_gold_store)\n",
    "                predictions = torch.argmax(predictions,dim=-1)\n",
    "                print(predictions.shape,references.shape)\n",
    "                y_pred = predictions.detach().cpu().clone().numpy()\n",
    "                y_true = references.detach().cpu().clone().numpy()\n",
    "                print(y_pred.shape,y_true.shape)\n",
    "                eval_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "                print('-'*100)\n",
    "                print(eval_f1)\n",
    "                print(f'Epoch {epoch} val loss {val_loss/len(final_eval_loader)}')\n",
    "                if val_loss/len(final_eval_loader) < best_loss:\n",
    "                    best_loss = val_loss/len(final_eval_loader)\n",
    "                    best_f1 = eval_f1['f1']\n",
    "                    torch.save(model.state_dict(),\"saved_model/adapter_pretrained_amazon.bin\")\n",
    "                print('-'*100)\n",
    "        \n",
    "                model.train()\n",
    "                epoch_loss = 0.0\n",
    "                for step, batch in enumerate(final_train_loader):\n",
    "                    data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(data)\n",
    "                    loss = fct(out.view(-1,model.num_labels),\\\n",
    "                                               data['labels'].view(-1))\n",
    "                    epoch_loss = epoch_loss + loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                print(f'Epoch {epoch} training loss {epoch_loss/len(final_train_loader)}')\n",
    "                print('**************************************************************************')\n",
    "            print(f'Best F1 score{best_f1},{best_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(data):\n",
    "    \n",
    "        model = PretrainedAdapterSequenceModel(2,False)\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(\"Test/adapter_pretrained_amazon.bin\"))\n",
    "        model.eval()\n",
    "        token_predictions_store = []\n",
    "        token_gold_store = []\n",
    "        for step, batch in enumerate(data):\n",
    "                    with torch.no_grad():\n",
    "                        data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                        out = model(data)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(data['labels'])\n",
    "                        loss = fct(out.view(-1,model.num_labels),\\\n",
    "                                                   data['labels'].view(-1))\n",
    "                        loss = loss + loss.item()\n",
    "\n",
    "        predictions = torch.vstack(token_predictions_store)\n",
    "        references = torch.hstack(token_gold_store)\n",
    "        predictions = torch.argmax(predictions,dim=-1)\n",
    "        print(predictions.shape,references.shape)\n",
    "        y_pred = predictions.detach().cpu().clone().numpy()\n",
    "        y_true = references.detach().cpu().clone().numpy()\n",
    "        test_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "        print(f'Test F1 score {test_f1}')\n",
    "        return test_f1['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_adapter(final_train_loader,final_eval_loader):\n",
    "            model = PretrainedAdapterSequenceModel(2)\n",
    "            model.cuda()\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped_parameters = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in model.named_parameters() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in model.named_parameters() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "            fct_loss = CrossEntropyLoss()\n",
    "            scheduler = ExponentialLR(optimizer=optimizer,gamma=0.99,last_epoch=-1,verbose=True)\n",
    "            \n",
    "            best_loss = 1e5\n",
    "            best_f1 = -1\n",
    "            for epoch in range(10):\n",
    "                print(f'EPOCH NO: {epoch}')\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                token_predictions_store = []\n",
    "                token_gold_store = []\n",
    "                for step, batch in enumerate(final_eval_loader):\n",
    "                    with torch.no_grad():\n",
    "                        data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                        out = model(data)\n",
    "                        out_preds = torch.argmax(out.logits,dim=1)\n",
    "                        token_predictions_store.append(out_preds)\n",
    "                        token_gold_store.append(data['labels'])\n",
    "                        loss = fct(out.logits,data['labels'])\n",
    "                        val_loss = val_loss + loss.item()\n",
    "\n",
    "                predictions = torch.hstack(token_predictions_store)\n",
    "                references = torch.hstack(token_gold_store)\n",
    "                #predictions = torch.argmax(predictions,dim=-1)\n",
    "                print(predictions.shape,references.shape)\n",
    "                y_pred = predictions.detach().cpu().clone().numpy()\n",
    "                y_true = references.detach().cpu().clone().numpy()\n",
    "                print(y_pred.shape,y_true.shape)\n",
    "                eval_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "                print('-'*100)\n",
    "                print(eval_f1)\n",
    "                print(f'Epoch {epoch} val loss {val_loss/len(final_eval_loader)}')\n",
    "                if best_f1 < eval_f1['f1']:\n",
    "                    best_loss = val_loss/len(final_eval_loader)\n",
    "                    best_f1 = eval_f1['f1']\n",
    "                    torch.save(model.state_dict(),\"Test/adapter_pretrained_amazon.bin\")\n",
    "                print('-'*100)\n",
    "        \n",
    "                model.train()\n",
    "                epoch_loss = 0.0\n",
    "                for step, batch in enumerate(final_train_loader):\n",
    "                    data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(data)\n",
    "                    loss = fct(out.logits,data['labels'])\n",
    "                    epoch_loss = epoch_loss + loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                print(f'Epoch {epoch} training loss {epoch_loss/len(final_train_loader)}')\n",
    "                print('**************************************************************************')\n",
    "            print(f'Best F1 score{best_f1},{best_loss}')\n",
    "            return model\n",
    "\n",
    "def run_test_adapter(data):\n",
    "    \n",
    "        model = PretrainedAdapterSequenceModel(2)\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(\"Test/adapter_pretrained_amazon.bin\"))\n",
    "        model.eval()\n",
    "        token_predictions_store = []\n",
    "        token_gold_store = []\n",
    "        for step, batch in enumerate(data):\n",
    "                    with torch.no_grad():\n",
    "                        data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                        out = model(data)\n",
    "                        out_preds = torch.argmax(out.logits,dim=1)\n",
    "                        token_predictions_store.append(out_preds)\n",
    "                        token_gold_store.append(data['labels'])\n",
    "                        loss = fct(out.logits,data['labels'])\n",
    "                        loss = loss + loss.item()\n",
    "\n",
    "        predictions = torch.hstack(token_predictions_store)\n",
    "        references = torch.hstack(token_gold_store)\n",
    "        print(predictions.shape,references.shape)\n",
    "        y_pred = predictions.detach().cpu().clone().numpy()\n",
    "        y_true = references.detach().cpu().clone().numpy()\n",
    "        test_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "        print(f'Test F1 score {test_f1}')\n",
    "        return test_f1['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline2():\n",
    "        seed = np.random.randint(0,1000000)\n",
    "        fix_all_seeds(seed)\n",
    "        dataset = load_dataset('csv',delimiter=\"\\t\",data_files='books/review_labels.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "        dataset_src = dataset.train_test_split(0.2,shuffle=False)\n",
    "\n",
    "        dataset = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/review_labels.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "        dataset_trg = dataset.train_test_split(0.2,shuffle=False)\n",
    "\n",
    "        processed_datasets_src = dataset_src.map(preprocess_function,batched=True,\\\n",
    "                                  desc=\"Running tokenizer on dataset\",)\n",
    "\n",
    "        processed_datasets_trg = dataset_trg.map(preprocess_function,batched=True,\\\n",
    "                                  desc=\"Running tokenizer on dataset\",)\n",
    "\n",
    "        processed_datasets_src.remove_columns_([\"text\"])\n",
    "        processed_datasets_trg.remove_columns_([\"text\"])\n",
    "\n",
    "\n",
    "        train_dataloader_src =DataLoader(processed_datasets_src['train'],\\\n",
    "                                                     collate_fn=default_data_collator,\\\n",
    "                                                     batch_size =16,drop_last=True)\n",
    "        eval_dataloader_src = DataLoader(processed_datasets_src['test'],\\\n",
    "                                         collate_fn=default_data_collator,\\\n",
    "                                         batch_size = 16,drop_last=True)\n",
    "        test_dataloader_tgt = DataLoader(processed_datasets_trg['test'],\\\n",
    "                                         collate_fn=default_data_collator,\\\n",
    "                                         batch_size = 16,drop_last=True)\n",
    "        m = run_train_adapter(train_dataloader_src,eval_dataloader_src)\n",
    "        return run_test_adapter(test_dataloader_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = baseline2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.base_model.save_all_adapters(\"Test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = BertModelWithHeads.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lang_config = PfeifferInvConfig(inv_adapter='nice')\n",
    "#task_config = PfeifferConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.load_adapter(\"Test/mlm/\",set_active=False,overwrite_ok=True,with_head=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.load_adapter(\"Test/sentiment/\",set_active=False,overwrite_ok=True,with_head=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.load_head(\"Test/sentiment/sentiment/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.base_model.save_all_heads(\"Test/sentiment/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac15194674ce426286464ed8f4b06adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae83f03ff6749148b4321f8010b1a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9531a61da7e433998d7cc7608067428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f3dc0d1a07446eb9312aab333a7f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c1def11c7548798b571f72552a95e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb80db292e743ebac5baaa0291d0b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 1 to 1.0000e-04.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.0}\n",
      "Epoch 0 val loss 0.7054719877243042\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.9000e-05.\n",
      "Adjusting learning rate of group 1 to 9.9000e-05.\n",
      "Epoch 0 training loss 0.4834526439756155\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8781725888324872}\n",
      "Epoch 1 val loss 0.31869212925434115\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.8010e-05.\n",
      "Adjusting learning rate of group 1 to 9.8010e-05.\n",
      "Epoch 1 training loss 0.25198290176689625\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8781725888324872}\n",
      "Epoch 2 val loss 0.34141191199421883\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.7030e-05.\n",
      "Adjusting learning rate of group 1 to 9.7030e-05.\n",
      "Epoch 2 training loss 0.21069680355489254\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8949999999999999}\n",
      "Epoch 3 val loss 0.3235421653091908\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.6060e-05.\n",
      "Adjusting learning rate of group 1 to 9.6060e-05.\n",
      "Epoch 3 training loss 0.16727104713208973\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8791773778920309}\n",
      "Epoch 4 val loss 0.38233789563179016\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.5099e-05.\n",
      "Adjusting learning rate of group 1 to 9.5099e-05.\n",
      "Epoch 4 training loss 0.14133232650347055\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8972431077694235}\n",
      "Epoch 5 val loss 0.38611583031713964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.4148e-05.\n",
      "Adjusting learning rate of group 1 to 9.4148e-05.\n",
      "Epoch 5 training loss 0.11067861623130738\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9027431421446384}\n",
      "Epoch 6 val loss 0.39838357305154204\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.3207e-05.\n",
      "Adjusting learning rate of group 1 to 9.3207e-05.\n",
      "Epoch 6 training loss 0.10284878889564425\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9056603773584906}\n",
      "Epoch 7 val loss 0.412337968791835\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.2274e-05.\n",
      "Adjusting learning rate of group 1 to 9.2274e-05.\n",
      "Epoch 7 training loss 0.08025625949841925\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9046454767726162}\n",
      "Epoch 8 val loss 0.46374134230893105\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.1352e-05.\n",
      "Adjusting learning rate of group 1 to 9.1352e-05.\n",
      "Epoch 8 training loss 0.054389980966225264\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8997668997668998}\n",
      "Epoch 9 val loss 0.43659291058778765\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.0438e-05.\n",
      "Adjusting learning rate of group 1 to 9.0438e-05.\n",
      "Epoch 9 training loss 0.07097497420851141\n",
      "**************************************************************************\n",
      "Best F1 score0.9056603773584906,0.412337968791835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8804347826086957}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661d4633b19e4158aff75fee3de1608e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41963f39f2d1405ba04f4af8acb3f874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cd8a91273645539aba4dcb6e493cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e1ae30b1f5450ba1cb115a06c6e752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca55d699b8e431a93ffd6aa97e9981e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832443220cd7419599baacd296aeb90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 1 to 1.0000e-04.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6885245901639345}\n",
      "Epoch 0 val loss 0.6939820575714112\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.9000e-05.\n",
      "Adjusting learning rate of group 1 to 9.9000e-05.\n",
      "Epoch 0 training loss 0.4965819142758846\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8526315789473684}\n",
      "Epoch 1 val loss 0.374863141477108\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.8010e-05.\n",
      "Adjusting learning rate of group 1 to 9.8010e-05.\n",
      "Epoch 1 training loss 0.2659211803227663\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8759493670886076}\n",
      "Epoch 2 val loss 0.3061171412467957\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.7030e-05.\n",
      "Adjusting learning rate of group 1 to 9.7030e-05.\n",
      "Epoch 2 training loss 0.21464396534487606\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8762886597938143}\n",
      "Epoch 3 val loss 0.3441607244312763\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.6060e-05.\n",
      "Adjusting learning rate of group 1 to 9.6060e-05.\n",
      "Epoch 3 training loss 0.17413422102108597\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8779220779220779}\n",
      "Epoch 4 val loss 0.38300666108727455\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.5099e-05.\n",
      "Adjusting learning rate of group 1 to 9.5099e-05.\n",
      "Epoch 4 training loss 0.1435449163801968\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8571428571428572}\n",
      "Epoch 5 val loss 0.48549940254539253\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.4148e-05.\n",
      "Adjusting learning rate of group 1 to 9.4148e-05.\n",
      "Epoch 5 training loss 0.13131899343803524\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.910411622276029}\n",
      "Epoch 6 val loss 0.3351198670826852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.3207e-05.\n",
      "Adjusting learning rate of group 1 to 9.3207e-05.\n",
      "Epoch 6 training loss 0.0868703231960535\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8978622327790975}\n",
      "Epoch 7 val loss 0.37492156578227875\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.2274e-05.\n",
      "Adjusting learning rate of group 1 to 9.2274e-05.\n",
      "Epoch 7 training loss 0.05941926485742442\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9107981220657277}\n",
      "Epoch 8 val loss 0.4010335787408985\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.1352e-05.\n",
      "Adjusting learning rate of group 1 to 9.1352e-05.\n",
      "Epoch 8 training loss 0.06823238330078311\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9055690072639224}\n",
      "Epoch 9 val loss 0.3871623346116394\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.0438e-05.\n",
      "Adjusting learning rate of group 1 to 9.0438e-05.\n",
      "Epoch 9 training loss 0.03157904990774114\n",
      "**************************************************************************\n",
      "Best F1 score0.9107981220657277,0.4010335787408985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8668555240793202}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546cd2bcf57842f0b0aa0f044851817c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937de66c79ba44be9d8143c455a0d8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead34b1bcc3f41b4888cf9c32786197a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47c953dcd2d4407937f920ad21acde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d850cfca23ae4240a7a33473bd12fd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009b0677291144fd9bac2d68bec3db9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 1 to 1.0000e-04.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6885245901639345}\n",
      "Epoch 0 val loss 0.7037584352493286\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.9000e-05.\n",
      "Adjusting learning rate of group 1 to 9.9000e-05.\n",
      "Epoch 0 training loss 0.49502692818641664\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8737373737373737}\n",
      "Epoch 1 val loss 0.3176766559481621\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.8010e-05.\n",
      "Adjusting learning rate of group 1 to 9.8010e-05.\n",
      "Epoch 1 training loss 0.25663593765348197\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8753180661577609}\n",
      "Epoch 2 val loss 0.31937060326337813\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.7030e-05.\n",
      "Adjusting learning rate of group 1 to 9.7030e-05.\n",
      "Epoch 2 training loss 0.21600598569959403\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8688946015424165}\n",
      "Epoch 3 val loss 0.34599967338144777\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.6060e-05.\n",
      "Adjusting learning rate of group 1 to 9.6060e-05.\n",
      "Epoch 3 training loss 0.17333884682506323\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8746803069053707}\n",
      "Epoch 4 val loss 0.39390250530093907\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.5099e-05.\n",
      "Adjusting learning rate of group 1 to 9.5099e-05.\n",
      "Epoch 4 training loss 0.13321510300505907\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8832487309644669}\n",
      "Epoch 5 val loss 0.3691121566481888\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.4148e-05.\n",
      "Adjusting learning rate of group 1 to 9.4148e-05.\n",
      "Epoch 5 training loss 0.12269959064666182\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9012048192771084}\n",
      "Epoch 6 val loss 0.3254096455406398\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.3207e-05.\n",
      "Adjusting learning rate of group 1 to 9.3207e-05.\n",
      "Epoch 6 training loss 0.11746499250875786\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8883495145631068}\n",
      "Epoch 7 val loss 0.38037759978324176\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.2274e-05.\n",
      "Adjusting learning rate of group 1 to 9.2274e-05.\n",
      "Epoch 7 training loss 0.0726391021348536\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.890547263681592}\n",
      "Epoch 8 val loss 0.4431832871772349\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.1352e-05.\n",
      "Adjusting learning rate of group 1 to 9.1352e-05.\n",
      "Epoch 8 training loss 0.047816452129627575\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8724489795918368}\n",
      "Epoch 9 val loss 0.6898542125965469\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.0438e-05.\n",
      "Adjusting learning rate of group 1 to 9.0438e-05.\n",
      "Epoch 9 training loss 0.0318303126009414\n",
      "**************************************************************************\n",
      "Best F1 score0.9012048192771084,0.3254096455406398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8538681948424068}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eed401a5064999a968397c98f342ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8c52253dda469dad480a17c42ad56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6860742fd94548289a03e3281bed0f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d70bc7cdcb43c2a02a1b86a2f30d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c74f688aa54f7fb25c4ae3d5542ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdc85afb3ce4a648c002d258092704c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 1 to 1.0000e-04.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.046511627906976744}\n",
      "Epoch 0 val loss 0.6933588981628418\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.9000e-05.\n",
      "Adjusting learning rate of group 1 to 9.9000e-05.\n",
      "Epoch 0 training loss 0.4717094188928604\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8608247422680413}\n",
      "Epoch 1 val loss 0.3308543482422829\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.8010e-05.\n",
      "Adjusting learning rate of group 1 to 9.8010e-05.\n",
      "Epoch 1 training loss 0.2559350369125605\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8675324675324675}\n",
      "Epoch 2 val loss 0.35478807270526885\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.7030e-05.\n",
      "Adjusting learning rate of group 1 to 9.7030e-05.\n",
      "Epoch 2 training loss 0.2093166050501168\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8860759493670887}\n",
      "Epoch 3 val loss 0.3198936213552952\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.6060e-05.\n",
      "Adjusting learning rate of group 1 to 9.6060e-05.\n",
      "Epoch 3 training loss 0.16894136752001943\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8838383838383838}\n",
      "Epoch 4 val loss 0.34165168210864066\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.5099e-05.\n",
      "Adjusting learning rate of group 1 to 9.5099e-05.\n",
      "Epoch 4 training loss 0.14440876439213754\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8939393939393939}\n",
      "Epoch 5 val loss 0.3921604409068823\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.4148e-05.\n",
      "Adjusting learning rate of group 1 to 9.4148e-05.\n",
      "Epoch 5 training loss 0.11737017995445058\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.894736842105263}\n",
      "Epoch 6 val loss 0.37750801341608164\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.3207e-05.\n",
      "Adjusting learning rate of group 1 to 9.3207e-05.\n",
      "Epoch 6 training loss 0.08501205276697874\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8980582524271845}\n",
      "Epoch 7 val loss 0.3578028150461614\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.2274e-05.\n",
      "Adjusting learning rate of group 1 to 9.2274e-05.\n",
      "Epoch 7 training loss 0.07884558213758282\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8904428904428904}\n",
      "Epoch 8 val loss 0.4614134988654405\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.1352e-05.\n",
      "Adjusting learning rate of group 1 to 9.1352e-05.\n",
      "Epoch 8 training loss 0.08647520549769978\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8955223880597014}\n",
      "Epoch 9 val loss 0.45608150492422284\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.0438e-05.\n",
      "Adjusting learning rate of group 1 to 9.0438e-05.\n",
      "Epoch 9 training loss 0.047680907427275085\n",
      "**************************************************************************\n",
      "Best F1 score0.8980582524271845,0.3578028150461614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8427299703264095}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349798e2cdb5415cbb105b49a93b38ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc03489ab9894389bd3fd6333ebe8978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5c9ce49a8c40799ed8a60d5521c9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a689f491237b4964bf372261cd0dbeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f0314a89414f258f6833f2d20d8756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b09302c97c04370b510dae4415849c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 1 to 1.0000e-04.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6885245901639345}\n",
      "Epoch 0 val loss 0.7010597085952759\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.9000e-05.\n",
      "Adjusting learning rate of group 1 to 9.9000e-05.\n",
      "Epoch 0 training loss 0.49464855559170245\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8737373737373737}\n",
      "Epoch 1 val loss 0.32422421902418136\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.8010e-05.\n",
      "Adjusting learning rate of group 1 to 9.8010e-05.\n",
      "Epoch 1 training loss 0.26447875823825595\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8746803069053707}\n",
      "Epoch 2 val loss 0.3357885767519474\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.7030e-05.\n",
      "Adjusting learning rate of group 1 to 9.7030e-05.\n",
      "Epoch 2 training loss 0.21839337231591344\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8791773778920309}\n",
      "Epoch 3 val loss 0.36732637718319894\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.6060e-05.\n",
      "Adjusting learning rate of group 1 to 9.6060e-05.\n",
      "Epoch 3 training loss 0.17442816007882356\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8838383838383838}\n",
      "Epoch 4 val loss 0.3613784060627222\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.5099e-05.\n",
      "Adjusting learning rate of group 1 to 9.5099e-05.\n",
      "Epoch 4 training loss 0.14279764702543615\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8866498740554156}\n",
      "Epoch 5 val loss 0.37634939447045324\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.4148e-05.\n",
      "Adjusting learning rate of group 1 to 9.4148e-05.\n",
      "Epoch 5 training loss 0.12362103880848735\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8827930174563592}\n",
      "Epoch 6 val loss 0.3673650398477912\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.3207e-05.\n",
      "Adjusting learning rate of group 1 to 9.3207e-05.\n",
      "Epoch 6 training loss 0.11138876372016966\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8867924528301887}\n",
      "Epoch 7 val loss 0.38512845419347286\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.2274e-05.\n",
      "Adjusting learning rate of group 1 to 9.2274e-05.\n",
      "Epoch 7 training loss 0.08142283561755903\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8883720930232558}\n",
      "Epoch 8 val loss 0.3928659680113196\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.1352e-05.\n",
      "Adjusting learning rate of group 1 to 9.1352e-05.\n",
      "Epoch 8 training loss 0.0683336611406412\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8878281622911695}\n",
      "Epoch 9 val loss 0.41107108515687285\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 9.0438e-05.\n",
      "Adjusting learning rate of group 1 to 9.0438e-05.\n",
      "Epoch 9 training loss 0.060596739361644725\n",
      "**************************************************************************\n",
      "Best F1 score0.8883720930232558,0.3928659680113196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8603351955307262}\n",
      "0.8608447334775116 0.012625181130485587\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for i in range(5):\n",
    "            output.append(baseline2())\n",
    "\n",
    "print(np.mean(output),np.std(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8636363636363636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((0.926-0.82786)/0.926)*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
