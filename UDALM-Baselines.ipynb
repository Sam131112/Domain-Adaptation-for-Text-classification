{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os\n",
    "import math\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "#model_checkpoint = \"allenai/scibert_scivocab_cased\"\n",
    "#model_checkpoint = \"roberta-large\"\n",
    "batch_size = 16\n",
    "from transformers import AutoConfig,AutoModel\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer,DataCollatorWithPadding,\\\n",
    "                                        TrainingArguments, Trainer,default_data_collator,AdamW\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)\n",
    "from torch.utils.data import DataLoader,RandomSampler\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "#tokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint,add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_metric\n",
    "import datasets\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = load_metric(\"f1\")\n",
    "pr_metric = load_metric('precision')\n",
    "re_metric = load_metric('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertPooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedSequenceModel(torch.nn.Module):\n",
    "    def __init__(self,labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = labels\n",
    "        self.base_model = AutoModel.from_pretrained(model_checkpoint,output_hidden_states=False,add_pooling_layer=False)\n",
    "        #Pretrained Using MLM and saved \n",
    "        #self.base_model.load_state_dict(torch.load(\"mlm_electronics/electronics_amazon.bin\"))\n",
    "        self.dropout = torch.nn.Dropout(self.base_model.config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(self.base_model.config.hidden_size,self.num_labels)\n",
    "        self._init_weights(self.classifier)\n",
    "        self.pooler = BertPooler(self.base_model.config)\n",
    "        self._init_weights(self.pooler)\n",
    "        \n",
    "    def _init_weights(self, modules):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        for module in modules.modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "                    if module.bias is not None:\n",
    "                        module.bias.data.zero_()\n",
    "                elif isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.bias.data.zero_()\n",
    "                    module.weight.data.fill_(1.0)\n",
    "        \n",
    "\n",
    "    def forward(self,data):\n",
    "        out = self.base_model(input_ids=data['input_ids'], \\\n",
    "                               attention_mask=data['attention_mask'])\n",
    "        out = self.pooler(out.last_hidden_state)\n",
    "        clf_out = self.classifier(self.dropout(out))\n",
    "        return clf_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "        result = tokenizer(examples['text'], padding=\"max_length\", max_length=512, truncation=True)\n",
    "        return result\n",
    "\n",
    "def compute_metrics(p):\n",
    "    f1_metric = load_metric(\"f1\")\n",
    "    pr_metric = load_metric('precision')\n",
    "    re_metric = load_metric('recall')\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    results1 = f1_metric.compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    results2 = pr_metric.compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    results3 = re_metric.compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    return {\n",
    "        \"precision\": results2[\"precision\"],\n",
    "        \"recall\": results3[\"recall\"],\n",
    "        \"f1\": results1[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88717"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline1():   \n",
    "            \n",
    "            seed = np.random.randint(0,1000000)\n",
    "            fix_all_seeds(seed)\n",
    "            dataset = load_dataset('csv',delimiter=\"\\t\",data_files='books/review_labels.csv')\n",
    "            dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "            dataset_src = dataset.train_test_split(0.2,shuffle=False)\n",
    "            \n",
    "            dataset = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/review_labels.csv')\n",
    "            dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "            dataset_trg = dataset.train_test_split(0.2,shuffle=False)\n",
    "            \n",
    "            processed_datasets_src = dataset_src.map(preprocess_function,batched=True,\\\n",
    "                                      desc=\"Running tokenizer on dataset\",)\n",
    "\n",
    "            processed_datasets_trg = dataset_trg.map(preprocess_function,batched=True,\\\n",
    "                                      desc=\"Running tokenizer on dataset\",)\n",
    "            \n",
    "            processed_datasets_src.remove_columns_([\"text\"])\n",
    "            processed_datasets_trg.remove_columns_([\"text\"])\n",
    "            \n",
    "            config = AutoConfig.from_pretrained(model_checkpoint,num_labels=2,)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,config=config,)\n",
    "            \n",
    "            args = TrainingArguments(\n",
    "                \"sanity-chunk\",\n",
    "                evaluation_strategy = \"epoch\",\n",
    "                learning_rate=5e-5,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                num_train_epochs=5,\n",
    "                weight_decay=0.01,\n",
    "                save_strategy=\"epoch\",\n",
    "                logging_steps=100,\n",
    "                overwrite_output_dir=True,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model = \"eval_f1\",\n",
    "                seed = seed,\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                    model,\n",
    "                    args,\n",
    "                    train_dataset=processed_datasets_src['train'],\n",
    "                    eval_dataset= processed_datasets_src['test'],\n",
    "                    data_collator=default_data_collator,\n",
    "                    tokenizer=tokenizer,\n",
    "                    compute_metrics=compute_metrics\n",
    "                )\n",
    "            \n",
    "            trainer.train(resume_from_checkpoint=None)\n",
    "            #p = trainer.predict(processed_datasets_src['test'])\n",
    "            p = trainer.predict(processed_datasets_trg['test'])\n",
    "            \n",
    "            y_hat = np.argmax(p.predictions,1)\n",
    "            y = p.label_ids\n",
    "            out = f1_metric.compute(predictions=y_hat,references=y)\n",
    "            return out['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(final_train_loader,final_eval_loader):\n",
    "            model = PretrainedSequenceModel(2)\n",
    "            model.cuda()\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped_parameters = [\n",
    "                    {\n",
    "                            \"params\": [p for n, p in model.named_parameters() \\\n",
    "                                       if not any(nd in n for nd in no_decay)],\n",
    "                            \"weight_decay\": 1e-2,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in model.named_parameters() \\\n",
    "                                   if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                        ]\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "            fct_loss = CrossEntropyLoss()\n",
    "            scheduler = ExponentialLR(optimizer=optimizer,gamma=0.85,last_epoch=-1,verbose=True)\n",
    "            \n",
    "            best_f1 = -1\n",
    "            for epoch in range(10):\n",
    "                print(f'EPOCH NO: {epoch}')\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                token_predictions_store = []\n",
    "                token_gold_store = []\n",
    "                for step, batch in enumerate(final_eval_loader):\n",
    "                    with torch.no_grad():\n",
    "                        data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                        out = model(data)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(batch['labels'])\n",
    "                        loss = fct(out.view(-1,model.num_labels),\\\n",
    "                                                   data['labels'].view(-1))\n",
    "                        val_loss = val_loss + loss.item()\n",
    "\n",
    "                predictions = torch.vstack(token_predictions_store)\n",
    "                references = torch.hstack(token_gold_store)\n",
    "                predictions = torch.argmax(predictions,dim=-1)\n",
    "                print(predictions.shape,references.shape)\n",
    "                y_pred = predictions.detach().cpu().clone().numpy()\n",
    "                y_true = references.detach().cpu().clone().numpy()\n",
    "                print(y_pred.shape,y_true.shape)\n",
    "                eval_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "                print('-'*100)\n",
    "                print(eval_f1)\n",
    "                print(f'Epoch {epoch} validation loss {val_loss/len(final_eval_loader)}')\n",
    "                if eval_f1['f1'] > best_f1:\n",
    "                    best_f1 = eval_f1['f1']\n",
    "                    torch.save(model.state_dict(),\"saved_model/pretrained_amazon.bin\")\n",
    "                print('-'*100)\n",
    "        \n",
    "                model.train()\n",
    "                epoch_loss = 0.0\n",
    "                for step, batch in enumerate(final_train_loader):\n",
    "                    data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(data)\n",
    "                    loss = fct(out.view(-1,model.num_labels),\\\n",
    "                                               data['labels'].view(-1))\n",
    "                    epoch_loss = epoch_loss + loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                print(f'Epoch {epoch} training loss {epoch_loss/len(final_train_loader)}')\n",
    "                print('**************************************************************************')\n",
    "            print(f'Best F1 score{best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(data):\n",
    "    \n",
    "        model = PretrainedSequenceModel(2)\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(\"saved_model/pretrained_amazon.bin\"))\n",
    "        model.eval()\n",
    "        token_predictions_store = []\n",
    "        token_gold_store = []\n",
    "        for step, batch in enumerate(data):\n",
    "                    with torch.no_grad():\n",
    "                        data = {'input_ids':batch['input_ids'].cuda(),\\\n",
    "                               'attention_mask':batch['attention_mask'].cuda(),\\\n",
    "                               'labels':batch['labels'].cuda()}\n",
    "                        out = model(data)\n",
    "                        token_predictions_store.append(out)\n",
    "                        token_gold_store.append(data['labels'])\n",
    "                        loss = fct(out.view(-1,model.num_labels),\\\n",
    "                                                   data['labels'].view(-1))\n",
    "                        loss = loss + loss.item()\n",
    "\n",
    "        predictions = torch.vstack(token_predictions_store)\n",
    "        references = torch.hstack(token_gold_store)\n",
    "        predictions = torch.argmax(predictions,dim=-1)\n",
    "        print(predictions.shape,references.shape)\n",
    "        y_pred = predictions.detach().cpu().clone().numpy()\n",
    "        y_true = references.detach().cpu().clone().numpy()\n",
    "        test_f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "        print(f'Test F1 score {test_f1}')\n",
    "        return test_f1['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline2():\n",
    "        seed = np.random.randint(0,1000000)\n",
    "        fix_all_seeds(seed)\n",
    "        dataset = load_dataset('csv',delimiter=\"\\t\",data_files='books/review_labels.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "        dataset_src = dataset.train_test_split(0.2,shuffle=False)\n",
    "\n",
    "        dataset = load_dataset('csv',delimiter=\"\\t\",data_files='electronics/review_labels.csv')\n",
    "        dataset = datasets.concatenate_datasets([dataset['train']])\n",
    "        dataset_trg = dataset.train_test_split(0.2,shuffle=False)\n",
    "\n",
    "        processed_datasets_src = dataset_src.map(preprocess_function,batched=True,\\\n",
    "                                  desc=\"Running tokenizer on dataset\",)\n",
    "\n",
    "        processed_datasets_trg = dataset_trg.map(preprocess_function,batched=True,\\\n",
    "                                  desc=\"Running tokenizer on dataset\",)\n",
    "\n",
    "        processed_datasets_src.remove_columns_([\"text\"])\n",
    "        processed_datasets_trg.remove_columns_([\"text\"])\n",
    "\n",
    "\n",
    "        train_dataloader_src =DataLoader(processed_datasets_src['train'],\\\n",
    "                                                     collate_fn=default_data_collator,\\\n",
    "                                                     batch_size = 16,drop_last=True)\n",
    "        eval_dataloader_src = DataLoader(processed_datasets_src['test'],\\\n",
    "                                         collate_fn=default_data_collator,\\\n",
    "                                         batch_size = 16,drop_last=True)\n",
    "        test_dataloader_tgt = DataLoader(processed_datasets_trg['test'],\\\n",
    "                                         collate_fn=default_data_collator,\\\n",
    "                                         batch_size = 16,drop_last=True)\n",
    "        run_train(train_dataloader_src,eval_dataloader_src)\n",
    "        return run_test(test_dataloader_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef09b3f14fed465e8fe937ebeb4cdd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8216b60f1b554168b8a3ae7d8e6cf9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dc6f062a914903954e6a92e457c269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d652acaad24aeb90eeecccc1cda4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77322375e4c47db920c483c1096bce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cd127348a5462580fd3b939a2cea02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-05.\n",
      "Adjusting learning rate of group 1 to 2.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6110056925996206}\n",
      "Epoch 0 validation loss 0.6952728962898255\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.7000e-05.\n",
      "Adjusting learning rate of group 1 to 1.7000e-05.\n",
      "Epoch 0 training loss 0.4088486244902015\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8939393939393939}\n",
      "Epoch 1 validation loss 0.27657682210206985\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.4450e-05.\n",
      "Adjusting learning rate of group 1 to 1.4450e-05.\n",
      "Epoch 1 training loss 0.14034541819244623\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8775510204081632}\n",
      "Epoch 2 validation loss 0.3523404040187597\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.2282e-05.\n",
      "Adjusting learning rate of group 1 to 1.2282e-05.\n",
      "Epoch 2 training loss 0.05658234738977626\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9009433962264151}\n",
      "Epoch 3 validation loss 0.3984926685038954\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.0440e-05.\n",
      "Adjusting learning rate of group 1 to 1.0440e-05.\n",
      "Epoch 3 training loss 0.03778812730219215\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9108433734939759}\n",
      "Epoch 4 validation loss 0.35525625041686\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 8.8741e-06.\n",
      "Adjusting learning rate of group 1 to 8.8741e-06.\n",
      "Epoch 4 training loss 0.022926255177007988\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9095127610208816}\n",
      "Epoch 5 validation loss 0.4221232162602246\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 7.5430e-06.\n",
      "Adjusting learning rate of group 1 to 7.5430e-06.\n",
      "Epoch 5 training loss 0.01325312051223591\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9019607843137255}\n",
      "Epoch 6 validation loss 0.4211978967883624\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 6.4115e-06.\n",
      "Adjusting learning rate of group 1 to 6.4115e-06.\n",
      "Epoch 6 training loss 0.005615691181737929\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.909952606635071}\n",
      "Epoch 7 validation loss 0.4188999039446935\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 5.4498e-06.\n",
      "Adjusting learning rate of group 1 to 5.4498e-06.\n",
      "Epoch 7 training loss 0.006634541782550514\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.900497512437811}\n",
      "Epoch 8 validation loss 0.4763989135972224\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.6323e-06.\n",
      "Adjusting learning rate of group 1 to 4.6323e-06.\n",
      "Epoch 8 training loss 0.005178742377320304\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.900990099009901}\n",
      "Epoch 9 validation loss 0.4817089683166705\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.9375e-06.\n",
      "Adjusting learning rate of group 1 to 3.9375e-06.\n",
      "Epoch 9 training loss 0.004578899879707023\n",
      "**************************************************************************\n",
      "Best F1 score0.9108433734939759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8367952522255193}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98a8f86158d465fb79f94a626f95958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebbdcfc9d6647eb85f3de96a323f13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276a9ee85cc94d1ab2248f0184fce58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4665c3066f9349c88f32993d7f6e3e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db74e1527d64859aebb07ff6e2b1ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8e4a61eb9e4738ac8c9b3225905ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-05.\n",
      "Adjusting learning rate of group 1 to 2.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6885245901639345}\n",
      "Epoch 0 validation loss 0.7026149797439575\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.7000e-05.\n",
      "Adjusting learning rate of group 1 to 1.7000e-05.\n",
      "Epoch 0 training loss 0.43040290985256435\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8865979381443299}\n",
      "Epoch 1 validation loss 0.32077419608831403\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.4450e-05.\n",
      "Adjusting learning rate of group 1 to 1.4450e-05.\n",
      "Epoch 1 training loss 0.1487919565103948\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8955223880597014}\n",
      "Epoch 2 validation loss 0.3685327572003007\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.2282e-05.\n",
      "Adjusting learning rate of group 1 to 1.2282e-05.\n",
      "Epoch 2 training loss 0.08741295415908099\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8960739030023094}\n",
      "Epoch 3 validation loss 0.36732880607247353\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.0440e-05.\n",
      "Adjusting learning rate of group 1 to 1.0440e-05.\n",
      "Epoch 3 training loss 0.021456899584736674\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9064748201438848}\n",
      "Epoch 4 validation loss 0.39246944620274005\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 8.8741e-06.\n",
      "Adjusting learning rate of group 1 to 8.8741e-06.\n",
      "Epoch 4 training loss 0.008080606192816048\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9116945107398567}\n",
      "Epoch 5 validation loss 0.43693010647082703\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 7.5430e-06.\n",
      "Adjusting learning rate of group 1 to 7.5430e-06.\n",
      "Epoch 5 training loss 0.0066913398343604056\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9082125603864734}\n",
      "Epoch 6 validation loss 0.4552405864931643\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 6.4115e-06.\n",
      "Adjusting learning rate of group 1 to 6.4115e-06.\n",
      "Epoch 6 training loss 0.007091049677692354\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9099756690997567}\n",
      "Epoch 7 validation loss 0.4397277908585966\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 5.4498e-06.\n",
      "Adjusting learning rate of group 1 to 5.4498e-06.\n",
      "Epoch 7 training loss 0.0059595375211210925\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9077669902912622}\n",
      "Epoch 8 validation loss 0.45052612920058893\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.6323e-06.\n",
      "Adjusting learning rate of group 1 to 4.6323e-06.\n",
      "Epoch 8 training loss 0.00691789252567105\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9095354523227384}\n",
      "Epoch 9 validation loss 0.4768601339380257\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.9375e-06.\n",
      "Adjusting learning rate of group 1 to 3.9375e-06.\n",
      "Epoch 9 training loss 0.00997244480124209\n",
      "**************************************************************************\n",
      "Best F1 score0.9116945107398567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8454810495626821}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29d1dbfea04446cb1b2980dcf23a6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401c339616054fe9851b8732b5038f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81422d886af147e59ad878de003d8ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec3c5f989824c2586427fa10ba0fdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ac88bc4158480ea100b62a74000a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799e65c24ea84d5ea90de1e7d405b812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-05.\n",
      "Adjusting learning rate of group 1 to 2.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6885245901639345}\n",
      "Epoch 0 validation loss 0.7080475807189941\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.7000e-05.\n",
      "Adjusting learning rate of group 1 to 1.7000e-05.\n",
      "Epoch 0 training loss 0.41725042566657067\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9068627450980393}\n",
      "Epoch 1 validation loss 0.27287818267941477\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.4450e-05.\n",
      "Adjusting learning rate of group 1 to 1.4450e-05.\n",
      "Epoch 1 training loss 0.13948053489904852\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9046454767726162}\n",
      "Epoch 2 validation loss 0.33355005798861387\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.2282e-05.\n",
      "Adjusting learning rate of group 1 to 1.2282e-05.\n",
      "Epoch 2 training loss 0.061099633702542636\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8817733990147782}\n",
      "Epoch 3 validation loss 0.47121054828166964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.0440e-05.\n",
      "Adjusting learning rate of group 1 to 1.0440e-05.\n",
      "Epoch 3 training loss 0.028928084346698597\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.896551724137931}\n",
      "Epoch 4 validation loss 0.44595218476373705\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 8.8741e-06.\n",
      "Adjusting learning rate of group 1 to 8.8741e-06.\n",
      "Epoch 4 training loss 0.010929745697649196\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9002433090024331}\n",
      "Epoch 5 validation loss 0.4460909537691623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 7.5430e-06.\n",
      "Adjusting learning rate of group 1 to 7.5430e-06.\n",
      "Epoch 5 training loss 0.00871786142757628\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8952380952380953}\n",
      "Epoch 6 validation loss 0.46627081418875604\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 6.4115e-06.\n",
      "Adjusting learning rate of group 1 to 6.4115e-06.\n",
      "Epoch 6 training loss 0.006065687256632373\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8932038834951457}\n",
      "Epoch 7 validation loss 0.4801898133708164\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 5.4498e-06.\n",
      "Adjusting learning rate of group 1 to 5.4498e-06.\n",
      "Epoch 7 training loss 0.005785760773578659\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8855721393034826}\n",
      "Epoch 8 validation loss 0.5307788267056458\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.6323e-06.\n",
      "Adjusting learning rate of group 1 to 4.6323e-06.\n",
      "Epoch 8 training loss 0.0047364003164693715\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9012048192771084}\n",
      "Epoch 9 validation loss 0.49066046555992215\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.9375e-06.\n",
      "Adjusting learning rate of group 1 to 3.9375e-06.\n",
      "Epoch 9 training loss 0.004301175503642298\n",
      "**************************************************************************\n",
      "Best F1 score0.9068627450980393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.7539936102236421}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54737d67a6a491f921baa246f8ce9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c25e6b9ef947f1939d91e199013fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cb3dfc9ace4d7fb8fa448fc7569276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0554f733dfad4beea2009e869a920def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5245629f9cbf484c828d87345cb68f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd4e63eb2264d419bf1607d2a728e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-05.\n",
      "Adjusting learning rate of group 1 to 2.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6864686468646864}\n",
      "Epoch 0 validation loss 0.6899154186248779\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.7000e-05.\n",
      "Adjusting learning rate of group 1 to 1.7000e-05.\n",
      "Epoch 0 training loss 0.4044397483766079\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8826530612244897}\n",
      "Epoch 1 validation loss 0.2964149688184261\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.4450e-05.\n",
      "Adjusting learning rate of group 1 to 1.4450e-05.\n",
      "Epoch 1 training loss 0.14863083199597896\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9027777777777779}\n",
      "Epoch 2 validation loss 0.30173891961574556\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.2282e-05.\n",
      "Adjusting learning rate of group 1 to 1.2282e-05.\n",
      "Epoch 2 training loss 0.06509952545864507\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9007263922518159}\n",
      "Epoch 3 validation loss 0.3777157433424145\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.0440e-05.\n",
      "Adjusting learning rate of group 1 to 1.0440e-05.\n",
      "Epoch 3 training loss 0.01904219391522929\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8936170212765958}\n",
      "Epoch 4 validation loss 0.46376314155291765\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 8.8741e-06.\n",
      "Adjusting learning rate of group 1 to 8.8741e-06.\n",
      "Epoch 4 training loss 0.01348758613341488\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8967254408060453}\n",
      "Epoch 5 validation loss 0.4678875791374594\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 7.5430e-06.\n",
      "Adjusting learning rate of group 1 to 7.5430e-06.\n",
      "Epoch 5 training loss 0.009365074533852748\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9024390243902439}\n",
      "Epoch 6 validation loss 0.46836570215644313\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 6.4115e-06.\n",
      "Adjusting learning rate of group 1 to 6.4115e-06.\n",
      "Epoch 6 training loss 0.00534772498649545\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9086538461538461}\n",
      "Epoch 7 validation loss 0.46660963649395854\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 5.4498e-06.\n",
      "Adjusting learning rate of group 1 to 5.4498e-06.\n",
      "Epoch 7 training loss 0.0049953433539485555\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9055690072639224}\n",
      "Epoch 8 validation loss 0.48201165078906344\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.6323e-06.\n",
      "Adjusting learning rate of group 1 to 4.6323e-06.\n",
      "Epoch 8 training loss 0.0049747234815731645\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8953771289537713}\n",
      "Epoch 9 validation loss 0.5079212323983665\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.9375e-06.\n",
      "Adjusting learning rate of group 1 to 3.9375e-06.\n",
      "Epoch 9 training loss 0.004138669819803908\n",
      "**************************************************************************\n",
      "Best F1 score0.9086538461538461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.8181818181818182}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-34360c82fb13fe72\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-34360c82fb13fe72/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a87bce4c2eb4495bffcd85de07be243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ce991164e62b0c8\n",
      "Reusing dataset csv (/ukp-storage-1/sarkar/.cache/huggingface/datasets/csv/default-6ce991164e62b0c8/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2440998b24bb490bab9e1f405e669277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881e9c18403448068c6c5343e32d3d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32745244616e4973848fe3858530f409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbda2b93d0f43e6ba7ff63ca0acb802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48987daba3344c41842e66a23a85fa3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-05.\n",
      "Adjusting learning rate of group 1 to 2.0000e-05.\n",
      "EPOCH NO: 0\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.6298932384341637}\n",
      "Epoch 0 validation loss 0.6988060665130615\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.7000e-05.\n",
      "Adjusting learning rate of group 1 to 1.7000e-05.\n",
      "Epoch 0 training loss 0.4637314909696579\n",
      "**************************************************************************\n",
      "EPOCH NO: 1\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8797953964194373}\n",
      "Epoch 1 validation loss 0.33085944324731825\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.4450e-05.\n",
      "Adjusting learning rate of group 1 to 1.4450e-05.\n",
      "Epoch 1 training loss 0.1726688768528402\n",
      "**************************************************************************\n",
      "EPOCH NO: 2\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8834951456310681}\n",
      "Epoch 2 validation loss 0.30699447914958\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.2282e-05.\n",
      "Adjusting learning rate of group 1 to 1.2282e-05.\n",
      "Epoch 2 training loss 0.09315900574903935\n",
      "**************************************************************************\n",
      "EPOCH NO: 3\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8985507246376812}\n",
      "Epoch 3 validation loss 0.3506039319932461\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 1.0440e-05.\n",
      "Adjusting learning rate of group 1 to 1.0440e-05.\n",
      "Epoch 3 training loss 0.03999918542103842\n",
      "**************************************************************************\n",
      "EPOCH NO: 4\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.8986175115207374}\n",
      "Epoch 4 validation loss 0.3624468054715544\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 8.8741e-06.\n",
      "Adjusting learning rate of group 1 to 8.8741e-06.\n",
      "Epoch 4 training loss 0.023267922983504833\n",
      "**************************************************************************\n",
      "EPOCH NO: 5\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9033816425120773}\n",
      "Epoch 5 validation loss 0.373991182516329\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 7.5430e-06.\n",
      "Adjusting learning rate of group 1 to 7.5430e-06.\n",
      "Epoch 5 training loss 0.01324030119460076\n",
      "**************************************************************************\n",
      "EPOCH NO: 6\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9099756690997567}\n",
      "Epoch 6 validation loss 0.4188453961070627\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 6.4115e-06.\n",
      "Adjusting learning rate of group 1 to 6.4115e-06.\n",
      "Epoch 6 training loss 0.0047943442879477514\n",
      "**************************************************************************\n",
      "EPOCH NO: 7\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9077669902912622}\n",
      "Epoch 7 validation loss 0.4371958176442422\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 5.4498e-06.\n",
      "Adjusting learning rate of group 1 to 5.4498e-06.\n",
      "Epoch 7 training loss 0.0032530055759707466\n",
      "**************************************************************************\n",
      "EPOCH NO: 8\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9099756690997567}\n",
      "Epoch 8 validation loss 0.4611329659726471\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 4.6323e-06.\n",
      "Adjusting learning rate of group 1 to 4.6323e-06.\n",
      "Epoch 8 training loss 0.0025777227251091973\n",
      "**************************************************************************\n",
      "EPOCH NO: 9\n",
      "torch.Size([400]) torch.Size([400])\n",
      "(400,) (400,)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'f1': 0.9033816425120773}\n",
      "Epoch 9 validation loss 0.4583446924132295\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Adjusting learning rate of group 0 to 3.9375e-06.\n",
      "Adjusting learning rate of group 1 to 3.9375e-06.\n",
      "Epoch 9 training loss 0.002177059173118323\n",
      "**************************************************************************\n",
      "Best F1 score0.9099756690997567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['bert.pooler.dense.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) torch.Size([400])\n",
      "Test F1 score {'f1': 0.7899686520376177}\n",
      "0.808884076446256 0.033393342865731174\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for i in range(5):\n",
    "            output.append(baseline2())\n",
    "\n",
    "print(np.mean(output),np.std(output))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.59827213822894"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((0.926-0.82786)/0.926)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
